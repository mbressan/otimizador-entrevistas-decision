{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf331b43",
   "metadata": {},
   "source": [
    "# Pipeline de Treinamento do Modelo de Otimiza√ß√£o de Entrevistas\n",
    "\n",
    "Este notebook implementa o pipeline completo de Machine Learning para o sistema de otimiza√ß√£o de entrevistas com modelo aprimorado:\n",
    "\n",
    "1. **Carregamento e Jun√ß√£o de Dados** - Importar vagas, candidatos e prospects da base completa\n",
    "2. **Engenharia de Features Avan√ßadas** - Criar features inteligentes que aproveitam todos os dados dispon√≠veis\n",
    "3. **Treinamento do Modelo Aprimorado** - Treinar um classificador com 7 features avan√ßadas\n",
    "4. **Avalia√ß√£o e Teste** - Validar a performance e testar com exemplos reais\n",
    "5. **Serializa√ß√£o** - Salvar o modelo treinado para produ√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822d4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9996ac6",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Explora√ß√£o dos Dados\n",
    "\n",
    "Vamos carregar os tr√™s arquivos JSON e entender a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9039008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando base completa de dados...\n",
      "Vagas carregadas: 14081\n",
      "Candidatos carregados: 42482\n",
      "Prospects carregados: 14222\n",
      "\n",
      "=== Estrutura das Vagas ===\n",
      "Chaves principais: ['informacoes_basicas', 'perfil_vaga', 'beneficios']\n",
      "Informa√ß√µes b√°sicas: ['data_requicisao', 'limite_esperado_para_contratacao', 'titulo_vaga', 'vaga_sap', 'cliente', 'solicitante_cliente', 'empresa_divisao', 'requisitante', 'analista_responsavel', 'tipo_contratacao', 'prazo_contratacao', 'objetivo_vaga', 'prioridade_vaga', 'origem_vaga', 'superior_imediato', 'nome', 'telefone']\n",
      "Perfil vaga: ['pais', 'estado', 'cidade', 'bairro', 'regiao', 'local_trabalho', 'vaga_especifica_para_pcd', 'faixa_etaria', 'horario_trabalho', 'nivel profissional', 'nivel_academico', 'nivel_ingles', 'nivel_espanhol', 'outro_idioma', 'areas_atuacao', 'principais_atividades', 'competencia_tecnicas_e_comportamentais', 'demais_observacoes', 'viagens_requeridas', 'equipamentos_necessarios']\n"
     ]
    }
   ],
   "source": [
    "# Definir caminhos dos arquivos\n",
    "data_path = Path(\"../data\")\n",
    "\n",
    "# Usar a base completa de dados para treinamento\n",
    "vagas_dev_path = data_path / \"vagas.json\"\n",
    "applicants_dev_path = data_path / \"applicants.json\"\n",
    "prospects_dev_path = data_path / \"prospects.json\"\n",
    "\n",
    "# Fun√ß√£o para carregar dados JSON\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Carrega dados de um arquivo JSON\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Carregar base completa de dados\n",
    "print(\"Carregando base completa de dados...\")\n",
    "vagas_data = load_json_data(vagas_dev_path)\n",
    "applicants_data = load_json_data(applicants_dev_path)\n",
    "prospects_data = load_json_data(prospects_dev_path)\n",
    "\n",
    "print(f\"Vagas carregadas: {len(vagas_data)}\")\n",
    "print(f\"Candidatos carregados: {len(applicants_data)}\")\n",
    "print(f\"Prospects carregados: {len(prospects_data)}\")\n",
    "\n",
    "# Explorar estrutura dos dados\n",
    "print(\"\\n=== Estrutura das Vagas ===\")\n",
    "if vagas_data:\n",
    "    vaga_exemplo = list(vagas_data.values())[0]\n",
    "    print(\"Chaves principais:\", list(vaga_exemplo.keys()))\n",
    "    print(\"Informa√ß√µes b√°sicas:\", list(vaga_exemplo.get('informacoes_basicas', {}).keys()))\n",
    "    print(\"Perfil vaga:\", list(vaga_exemplo.get('perfil_vaga', {}).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34be6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados das vagas...\n",
      "Processando dados dos candidatos...\n",
      "\n",
      "DataFrame Vagas: (14081, 13)\n",
      "DataFrame Candidatos: (42482, 12)\n",
      "\n",
      "=== Primeiras vagas ===\n",
      "  id_vaga             titulo_vaga nivel_profissional  \\\n",
      "0    5185        Operation Lead -             S√™nior   \n",
      "1    5184  Consultor PP/QM S√™nior             S√™nior   \n",
      "2    5183   ANALISTA PL/JR C/ SQL           Analista   \n",
      "\n",
      "                       areas_atuacao  \n",
      "0       TI - Sistemas e Ferramentas-  \n",
      "1  TI - Desenvolvimento/Programa√ß√£o-  \n",
      "2       TI - Sistemas e Ferramentas-  \n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para normalizar dados das vagas\n",
    "def normalize_vagas(vagas_data):\n",
    "    \"\"\"Normaliza os dados das vagas para um formato estruturado\"\"\"\n",
    "    vagas_list = []\n",
    "    \n",
    "    for vaga_id, vaga_info in vagas_data.items():\n",
    "        try:\n",
    "            basic_info = vaga_info.get('informacoes_basicas', {})\n",
    "            profile_info = vaga_info.get('perfil_vaga', {})\n",
    "            \n",
    "            normalized_vaga = {\n",
    "                'id_vaga': vaga_id,\n",
    "                'titulo_vaga': basic_info.get('titulo_vaga', ''),\n",
    "                'cliente': basic_info.get('cliente', ''),\n",
    "                'tipo_contratacao': basic_info.get('tipo_contratacao', ''),\n",
    "                'nivel_profissional': profile_info.get('nivel profissional', ''),\n",
    "                'nivel_academico': profile_info.get('nivel_academico', ''),\n",
    "                'nivel_ingles': profile_info.get('nivel_ingles', ''),\n",
    "                'areas_atuacao': profile_info.get('areas_atuacao', ''),\n",
    "                'competencias_tecnicas_requeridas': profile_info.get('competencia_tecnicas_e_comportamentais', ''),\n",
    "                'principais_atividades': profile_info.get('principais_atividades', ''),\n",
    "                'pais': profile_info.get('pais', ''),\n",
    "                'estado': profile_info.get('estado', ''),\n",
    "                'cidade': profile_info.get('cidade', '')\n",
    "            }\n",
    "            vagas_list.append(normalized_vaga)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar vaga {vaga_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(vagas_list)\n",
    "\n",
    "# Fun√ß√£o para normalizar dados dos candidatos\n",
    "def normalize_applicants(applicants_data):\n",
    "    \"\"\"Normaliza os dados dos candidatos\"\"\"\n",
    "    applicants_list = []\n",
    "    \n",
    "    for candidate_id, candidate_info in applicants_data.items():\n",
    "        try:\n",
    "            basic_info = candidate_info.get('infos_basicas', {})\n",
    "            personal_info = candidate_info.get('informacoes_pessoais', {})\n",
    "            academic_info = candidate_info.get('formacao_academica', {})\n",
    "            professional_info = candidate_info.get('informacoes_profissionais', {})\n",
    "            \n",
    "            normalized_candidate = {\n",
    "                'codigo_candidato': candidate_id,\n",
    "                'nome': basic_info.get('nome', ''),\n",
    "                'email': basic_info.get('email', ''),\n",
    "                'telefone': basic_info.get('telefone', ''),\n",
    "                'data_nascimento': personal_info.get('data_nascimento', ''),\n",
    "                'estado_civil': personal_info.get('estado_civil', ''),\n",
    "                'pcd': personal_info.get('pcd', ''),\n",
    "                'nivel_academico': academic_info.get('nivel_academico', '') if academic_info else '',\n",
    "                'area_formacao': academic_info.get('area_formacao', '') if academic_info else '',\n",
    "                'nivel_ingles': professional_info.get('nivel_ingles', '') if professional_info else '',\n",
    "                'conhecimentos_tecnicos': professional_info.get('conhecimentos_tecnicos', '') if professional_info else '',\n",
    "                'area_de_atuacao': professional_info.get('area_atuacao', '') if professional_info else ''\n",
    "            }\n",
    "            applicants_list.append(normalized_candidate)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar candidato {candidate_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(applicants_list)\n",
    "\n",
    "# Processar os dados\n",
    "print(\"Processando dados das vagas...\")\n",
    "df_vagas = normalize_vagas(vagas_data)\n",
    "\n",
    "print(\"Processando dados dos candidatos...\")\n",
    "df_candidates = normalize_applicants(applicants_data)\n",
    "\n",
    "print(f\"\\nDataFrame Vagas: {df_vagas.shape}\")\n",
    "print(f\"DataFrame Candidatos: {df_candidates.shape}\")\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\n=== Primeiras vagas ===\")\n",
    "print(df_vagas.head(3)[['id_vaga', 'titulo_vaga', 'nivel_profissional', 'areas_atuacao']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7faf4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados dos prospects...\n",
      "DataFrame Prospects: (53759, 8)\n",
      "\n",
      "Distribui√ß√£o da vari√°vel alvo 'contratado':\n",
      "contratado\n",
      "0    51001\n",
      "1     2758\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Taxa de contrata√ß√£o: 5.13%\n",
      "\n",
      "=== Situa√ß√µes dos candidatos ===\n",
      "situacao_candidado\n",
      "Prospect                          20021\n",
      "Encaminhado ao Requisitante       16122\n",
      "Inscrito                           3980\n",
      "N√£o Aprovado pelo Cliente          3492\n",
      "Contratado pela Decision           2758\n",
      "Desistiu                           2349\n",
      "N√£o Aprovado pelo RH               1765\n",
      "N√£o Aprovado pelo Requisitante      765\n",
      "Entrevista T√©cnica                  579\n",
      "Sem interesse nesta vaga            576\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para processar prospects e criar vari√°vel alvo\n",
    "def process_prospects(prospects_data):\n",
    "    \"\"\"Processa os dados de prospects para extrair pares candidato-vaga com situa√ß√£o\"\"\"\n",
    "    prospects_list = []\n",
    "    \n",
    "    for vaga_id, vaga_prospect in prospects_data.items():\n",
    "        vaga_titulo = vaga_prospect.get('titulo', '')\n",
    "        prospects = vaga_prospect.get('prospects', [])\n",
    "        \n",
    "        for prospect in prospects:\n",
    "            try:\n",
    "                # Extrair c√≥digo do candidato (remover prefixos se existirem)\n",
    "                codigo_candidato = prospect.get('codigo', '').strip()\n",
    "                situacao = prospect.get('situacao_candidado', '').strip()\n",
    "                \n",
    "                prospect_record = {\n",
    "                    'id_vaga': vaga_id,\n",
    "                    'codigo_candidato': codigo_candidato,\n",
    "                    'nome_candidato': prospect.get('nome', ''),\n",
    "                    'situacao_candidado': situacao,\n",
    "                    'data_candidatura': prospect.get('data_candidatura', ''),\n",
    "                    'comentario': prospect.get('comentario', ''),\n",
    "                    'recrutador': prospect.get('recrutador', ''),\n",
    "                    # Criar vari√°vel alvo bin√°ria\n",
    "                    'contratado': 1 if 'Contratado pela Decision' in situacao else 0\n",
    "                }\n",
    "                prospects_list.append(prospect_record)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar prospect da vaga {vaga_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(prospects_list)\n",
    "\n",
    "# Processar prospects\n",
    "print(\"Processando dados dos prospects...\")\n",
    "df_prospects = process_prospects(prospects_data)\n",
    "\n",
    "print(f\"DataFrame Prospects: {df_prospects.shape}\")\n",
    "print(f\"\\nDistribui√ß√£o da vari√°vel alvo 'contratado':\")\n",
    "print(df_prospects['contratado'].value_counts())\n",
    "print(f\"\\nTaxa de contrata√ß√£o: {df_prospects['contratado'].mean():.2%}\")\n",
    "\n",
    "# Visualizar algumas situa√ß√µes de candidatos\n",
    "print(\"\\n=== Situa√ß√µes dos candidatos ===\")\n",
    "print(df_prospects['situacao_candidado'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70043d88",
   "metadata": {},
   "source": [
    "## 2. Combina√ß√£o e Prepara√ß√£o dos Dados\n",
    "\n",
    "Vamos combinar os tr√™s DataFrames para criar um dataset unificado com pares candidato-vaga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6af761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinando os dados...\n",
      "Ap√≥s join com vagas: (53735, 20)\n",
      "Dataset final: (45071, 31)\n",
      "Taxa de contrata√ß√£o no dataset final: 5.00%\n",
      "\n",
      "=== Colunas dispon√≠veis ===\n",
      " 1. id_vaga\n",
      " 2. codigo_candidato\n",
      " 3. nome_candidato\n",
      " 4. situacao_candidado\n",
      " 5. data_candidatura\n",
      " 6. comentario\n",
      " 7. recrutador\n",
      " 8. contratado\n",
      " 9. titulo_vaga\n",
      "10. cliente\n",
      "11. tipo_contratacao\n",
      "12. nivel_profissional\n",
      "13. nivel_academico_vaga\n",
      "14. nivel_ingles_vaga\n",
      "15. areas_atuacao\n",
      "16. competencias_tecnicas_requeridas\n",
      "17. principais_atividades\n",
      "18. pais\n",
      "19. estado\n",
      "20. cidade\n",
      "21. nome\n",
      "22. email\n",
      "23. telefone\n",
      "24. data_nascimento\n",
      "25. estado_civil\n",
      "26. pcd\n",
      "27. nivel_academico_candidato\n",
      "28. area_formacao\n",
      "29. nivel_ingles_candidato\n",
      "30. conhecimentos_tecnicos\n",
      "31. area_de_atuacao\n",
      "\n",
      "=== Amostra dos dados combinados ===\n",
      "  id_vaga                                        titulo_vaga  \\\n",
      "0    4530                                CONSULTOR CONTROL M   \n",
      "1    4530                                CONSULTOR CONTROL M   \n",
      "2    4531  2021-2607395-PeopleSoft Application Engine-Dom...   \n",
      "3    4531  2021-2607395-PeopleSoft Application Engine-Dom...   \n",
      "4    4533  2021-2605708-Microfocus Application Life Cycle...   \n",
      "\n",
      "             nome_candidato           situacao_candidado  contratado  \n",
      "0               Jos√© Vieira  Encaminhado ao Requisitante           0  \n",
      "1  Srta. Isabela Cavalcante  Encaminhado ao Requisitante           0  \n",
      "2     Sra. Yasmin Fernandes     Contratado pela Decision           1  \n",
      "3            Alexia Barbosa  Encaminhado ao Requisitante           0  \n",
      "4            Arthur Almeida     Contratado pela Decision           1  \n"
     ]
    }
   ],
   "source": [
    "# Combinar os DataFrames\n",
    "print(\"Combinando os dados...\")\n",
    "\n",
    "# Primeiro, fazer join prospects com vagas\n",
    "df_combined = df_prospects.merge(\n",
    "    df_vagas, \n",
    "    on='id_vaga', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Ap√≥s join com vagas: {df_combined.shape}\")\n",
    "\n",
    "# Depois, fazer join com candidatos\n",
    "df_final = df_combined.merge(\n",
    "    df_candidates,\n",
    "    on='codigo_candidato',\n",
    "    how='inner',\n",
    "    suffixes=('_vaga', '_candidato')\n",
    ")\n",
    "\n",
    "print(f\"Dataset final: {df_final.shape}\")\n",
    "print(f\"Taxa de contrata√ß√£o no dataset final: {df_final['contratado'].mean():.2%}\")\n",
    "\n",
    "# Verificar colunas dispon√≠veis\n",
    "print(\"\\n=== Colunas dispon√≠veis ===\")\n",
    "for i, col in enumerate(df_final.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "# Visualizar algumas linhas\n",
    "print(\"\\n=== Amostra dos dados combinados ===\")\n",
    "sample_cols = ['id_vaga', 'titulo_vaga', 'nome_candidato', 'situacao_candidado', 'contratado']\n",
    "print(df_final[sample_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e2e17",
   "metadata": {},
   "source": [
    "## 3. Configura√ß√£o do Banco de Dados PostgreSQL\n",
    "\n",
    "Vamos criar estruturas de banco de dados PostgreSQL para armazenar os dados de forma eficiente e escal√°vel. O PostgreSQL oferece recursos avan√ßados como √≠ndices GIN para busca full-text e melhor performance para aplica√ß√µes em produ√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b587091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURA√á√ÉO DO BANCO DE DADOS POSTGRESQL\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== CONFIGURANDO BANCO DE DADOS POSTGRESQL ===\")\n",
    "\n",
    "# Configura√ß√µes do PostgreSQL - usando vari√°veis de ambiente\n",
    "PG_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'otimizador_entrevistas'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432))\n",
    "}\n",
    "\n",
    "print(f\"üîó Conectando ao PostgreSQL: {PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\")\n",
    "\n",
    "def create_pg_engine():\n",
    "    \"\"\"Cria engine SQLAlchemy para PostgreSQL\"\"\"\n",
    "    connection_string = f\"postgresql://{PG_CONFIG['user']}:{PG_CONFIG['password']}@{PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\"\n",
    "    return create_engine(connection_string)\n",
    "\n",
    "def create_database_schema():\n",
    "    \"\"\"Cria as tabelas do banco PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(\"üìã Criando tabelas PostgreSQL...\")\n",
    "        \n",
    "        # Tabela de Vagas\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS vagas (\n",
    "            id_vaga VARCHAR(50) PRIMARY KEY,\n",
    "            titulo_vaga TEXT,\n",
    "            cliente VARCHAR(200),\n",
    "            tipo_contratacao VARCHAR(100),\n",
    "            nivel_profissional VARCHAR(50),\n",
    "            nivel_academico VARCHAR(50),\n",
    "            nivel_ingles VARCHAR(50),\n",
    "            areas_atuacao TEXT,\n",
    "            competencias_tecnicas_requeridas TEXT,\n",
    "            principais_atividades TEXT,\n",
    "            pais VARCHAR(100),\n",
    "            estado VARCHAR(100),\n",
    "            cidade VARCHAR(100),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de Candidatos\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS candidatos (\n",
    "            codigo_candidato VARCHAR(50) PRIMARY KEY,\n",
    "            nome VARCHAR(200),\n",
    "            email VARCHAR(200),\n",
    "            telefone VARCHAR(50),\n",
    "            data_nascimento VARCHAR(20),\n",
    "            estado_civil VARCHAR(50),\n",
    "            pcd VARCHAR(10),\n",
    "            nivel_academico VARCHAR(50),\n",
    "            area_formacao VARCHAR(200),\n",
    "            nivel_ingles VARCHAR(50),\n",
    "            conhecimentos_tecnicos TEXT,\n",
    "            area_de_atuacao VARCHAR(200),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de Prospects (hist√≥rico de candidaturas)\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS prospects (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            id_vaga VARCHAR(50),\n",
    "            codigo_candidato VARCHAR(50),\n",
    "            nome_candidato VARCHAR(200),\n",
    "            situacao_candidado VARCHAR(200),\n",
    "            data_candidatura VARCHAR(20),\n",
    "            comentario TEXT,\n",
    "            recrutador VARCHAR(200),\n",
    "            contratado INTEGER,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (id_vaga) REFERENCES vagas(id_vaga) ON DELETE CASCADE,\n",
    "            FOREIGN KEY (codigo_candidato) REFERENCES candidatos(codigo_candidato) ON DELETE CASCADE\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de Predi√ß√µes (log das predi√ß√µes do ML)\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS predicoes (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            id_vaga VARCHAR(50),\n",
    "            codigo_candidato VARCHAR(50),\n",
    "            tech_match_score REAL,\n",
    "            academic_match VARCHAR(50),\n",
    "            english_match VARCHAR(50),\n",
    "            probabilidade_contratacao REAL,\n",
    "            predicao_contratado INTEGER,\n",
    "            modelo_versao VARCHAR(50),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (id_vaga) REFERENCES vagas(id_vaga) ON DELETE CASCADE,\n",
    "            FOREIGN KEY (codigo_candidato) REFERENCES candidatos(codigo_candidato) ON DELETE CASCADE\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # √çndices otimizados para PostgreSQL\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vagas_nivel ON vagas(nivel_profissional)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vagas_areas ON vagas USING gin(to_tsvector(\\'portuguese\\', areas_atuacao))')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_candidatos_area ON candidatos USING gin(to_tsvector(\\'portuguese\\', area_de_atuacao))')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_prospects_vaga ON prospects(id_vaga)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_prospects_candidato ON prospects(codigo_candidato)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_prospects_contratado ON prospects(contratado)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_predicoes_vaga ON predicoes(id_vaga)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_predicoes_data ON predicoes(created_at)')\n",
    "        \n",
    "        # Views √∫teis para PostgreSQL\n",
    "        cursor.execute('''\n",
    "        CREATE OR REPLACE VIEW vw_vagas_stats AS\n",
    "        SELECT \n",
    "            v.id_vaga,\n",
    "            v.titulo_vaga,\n",
    "            v.cliente,\n",
    "            v.nivel_profissional,\n",
    "            v.areas_atuacao,\n",
    "            COUNT(p.codigo_candidato) as total_candidatos,\n",
    "            SUM(p.contratado) as total_contratados,\n",
    "            CASE \n",
    "                WHEN COUNT(p.codigo_candidato) > 0 \n",
    "                THEN ROUND(AVG(p.contratado) * 100, 2) \n",
    "                ELSE 0 \n",
    "            END as taxa_contratacao\n",
    "        FROM vagas v\n",
    "        LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "        GROUP BY v.id_vaga, v.titulo_vaga, v.cliente, v.nivel_profissional, v.areas_atuacao\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE OR REPLACE VIEW vw_candidatos_ranking AS\n",
    "        SELECT \n",
    "            c.codigo_candidato,\n",
    "            c.nome,\n",
    "            c.area_de_atuacao,\n",
    "            c.nivel_academico,\n",
    "            COUNT(p.id_vaga) as total_candidaturas,\n",
    "            SUM(p.contratado) as total_contratacoes,\n",
    "            CASE \n",
    "                WHEN COUNT(p.id_vaga) > 0 \n",
    "                THEN ROUND(AVG(p.contratado) * 100, 2) \n",
    "                ELSE 0 \n",
    "            END as taxa_sucesso\n",
    "        FROM candidatos c\n",
    "        LEFT JOIN prospects p ON c.codigo_candidato = p.codigo_candidato\n",
    "        GROUP BY c.codigo_candidato, c.nome, c.area_de_atuacao, c.nivel_academico\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"‚úÖ Esquema PostgreSQL criado com sucesso!\")\n",
    "        \n",
    "        # Verificar tabelas criadas\n",
    "        cursor.execute(\"\"\"\n",
    "        SELECT tablename \n",
    "        FROM pg_tables \n",
    "        WHERE schemaname = 'public' \n",
    "        AND tablename IN ('vagas', 'candidatos', 'prospects', 'predicoes')\n",
    "        \"\"\")\n",
    "        tabelas = cursor.fetchall()\n",
    "        print(f\"üìä Tabelas criadas: {[t[0] for t in tabelas]}\")\n",
    "        \n",
    "        # Verificar views criadas\n",
    "        cursor.execute(\"\"\"\n",
    "        SELECT viewname \n",
    "        FROM pg_views \n",
    "        WHERE schemaname = 'public' \n",
    "        AND viewname LIKE 'vw_%'\n",
    "        \"\"\")\n",
    "        views = cursor.fetchall()\n",
    "        print(f\"üëÅÔ∏è  Views criadas: {[v[0] for v in views]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar esquema PostgreSQL: {e}\")\n",
    "        print(\"üí° Certifique-se de que:\")\n",
    "        print(\"   - PostgreSQL est√° rodando\")\n",
    "        print(\"   - Banco 'otimizador_entrevistas' existe\")\n",
    "        print(\"   - Credenciais est√£o corretas\")\n",
    "        print(\"   - psycopg2-binary est√° instalado: pip install psycopg2-binary\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Testar conex√£o e criar esquema\n",
    "try:\n",
    "    create_database_schema()\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Para configurar PostgreSQL:\")\n",
    "    print(\"1. Instale PostgreSQL: https://www.postgresql.org/download/\")\n",
    "    print(\"2. Crie o banco: createdb otimizador_entrevistas\")\n",
    "    print(\"3. Configure vari√°veis de ambiente:\")\n",
    "    print(\"   export DB_HOST=localhost\")\n",
    "    print(\"   export DB_NAME=otimizador_entrevistas\") \n",
    "    print(\"   export DB_USER=postgres\")\n",
    "    print(\"   export DB_PASSWORD=sua_senha\")\n",
    "    print(\"4. Instale driver: pip install psycopg2-binary sqlalchemy\")\n",
    "    print(f\"\\nErro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGRA√á√ÉO DOS DADOS JSON PARA POSTGRESQL\n",
    "def migrate_data_to_postgresql():\n",
    "    \"\"\"Migra os dados dos DataFrames para o banco PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        \n",
    "        print(\"üöÄ Migrando dados para PostgreSQL...\")\n",
    "        \n",
    "        # Verificar se j√° existem dados\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(\"SELECT COUNT(*) FROM vagas\").fetchone()\n",
    "            vagas_count = result[0]\n",
    "        \n",
    "        if vagas_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  Banco j√° cont√©m {vagas_count} vagas. Pulando migra√ß√£o.\")\n",
    "            print(\"   Para recriar os dados, execute: TRUNCATE TABLE prospects, candidatos, vagas RESTART IDENTITY CASCADE;\")\n",
    "            return\n",
    "        \n",
    "        # 1. Migrar Vagas\n",
    "        print(\"üìã Migrando vagas para PostgreSQL...\")\n",
    "        df_vagas_clean = df_vagas.fillna('')  # Tratar valores nulos\n",
    "        \n",
    "        # Usar pandas to_sql com PostgreSQL\n",
    "        rows_inserted = df_vagas_clean.to_sql(\n",
    "            'vagas', \n",
    "            engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            method='multi',  # Mais eficiente para grandes volumes\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"‚úÖ {len(df_vagas)} vagas migradas\")\n",
    "        \n",
    "        # 2. Migrar Candidatos\n",
    "        print(\"üë• Migrando candidatos para PostgreSQL...\")\n",
    "        df_candidates_clean = df_candidates.fillna('')  # Tratar valores nulos\n",
    "        \n",
    "        rows_inserted = df_candidates_clean.to_sql(\n",
    "            'candidatos', \n",
    "            engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"‚úÖ {len(df_candidates)} candidatos migrados\")\n",
    "        \n",
    "        # 3. Migrar Prospects\n",
    "        print(\"üéØ Migrando prospects para PostgreSQL...\")\n",
    "        df_prospects_clean = df_prospects.fillna('')  # Tratar valores nulos\n",
    "        \n",
    "        rows_inserted = df_prospects_clean.to_sql(\n",
    "            'prospects', \n",
    "            engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"‚úÖ {len(df_prospects)} prospects migrados\")\n",
    "        \n",
    "        # Verificar migra√ß√£o usando pandas\n",
    "        with engine.connect() as conn:\n",
    "            vagas_df = pd.read_sql(\"SELECT COUNT(*) as count FROM vagas\", conn)\n",
    "            candidatos_df = pd.read_sql(\"SELECT COUNT(*) as count FROM candidatos\", conn)\n",
    "            prospects_df = pd.read_sql(\"SELECT COUNT(*) as count FROM prospects\", conn)\n",
    "            contratados_df = pd.read_sql(\"SELECT COUNT(*) as count FROM prospects WHERE contratado = 1\", conn)\n",
    "            \n",
    "            vagas_db = vagas_df['count'].iloc[0]\n",
    "            candidatos_db = candidatos_df['count'].iloc[0]\n",
    "            prospects_db = prospects_df['count'].iloc[0]\n",
    "            contratados_db = contratados_df['count'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nüìä RESUMO DA MIGRA√á√ÉO POSTGRESQL:\")\n",
    "        print(f\"   Vagas no banco: {vagas_db:,}\")\n",
    "        print(f\"   Candidatos no banco: {candidatos_db:,}\")\n",
    "        print(f\"   Prospects no banco: {prospects_db:,}\")\n",
    "        print(f\"   Contratados: {contratados_db:,}\")\n",
    "        print(f\"   Taxa de contrata√ß√£o: {contratados_db/prospects_db:.2%}\")\n",
    "        \n",
    "        # Informa√ß√µes do banco PostgreSQL\n",
    "        with engine.connect() as conn:\n",
    "            # Tamanho das tabelas\n",
    "            size_query = \"\"\"\n",
    "            SELECT \n",
    "                schemaname,\n",
    "                tablename,\n",
    "                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
    "            FROM pg_tables \n",
    "            WHERE schemaname = 'public' \n",
    "            AND tablename IN ('vagas', 'candidatos', 'prospects', 'predicoes')\n",
    "            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n",
    "            \"\"\"\n",
    "            sizes_df = pd.read_sql(size_query, conn)\n",
    "            \n",
    "            print(f\"\\n\udcbe TAMANHO DAS TABELAS:\")\n",
    "            for _, row in sizes_df.iterrows():\n",
    "                print(f\"   {row['tablename']}: {row['size']}\")\n",
    "        \n",
    "        print(f\"\\nüéâ Migra√ß√£o PostgreSQL conclu√≠da com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na migra√ß√£o PostgreSQL: {e}\")\n",
    "        print(f\"üí° Verifique se o PostgreSQL est√° rodando e as credenciais est√£o corretas\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Executar migra√ß√£o PostgreSQL\n",
    "migrate_data_to_postgresql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN√á√ïES UTILIT√ÅRIAS PARA CONSULTAS POSTGRESQL\n",
    "def query_postgresql_stats():\n",
    "    \"\"\"Consulta estat√≠sticas do banco PostgreSQL usando pandas\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        print(\"=== ESTAT√çSTICAS DO BANCO POSTGRESQL ===\")\n",
    "        \n",
    "        # Estat√≠sticas b√°sicas usando pandas\n",
    "        with engine.connect() as conn:\n",
    "            # Contadores b√°sicos\n",
    "            stats_query = \"\"\"\n",
    "            SELECT \n",
    "                'Total de Vagas' as metrica,\n",
    "                (SELECT COUNT(*) FROM vagas) as valor\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Total de Candidatos',\n",
    "                (SELECT COUNT(*) FROM candidatos)\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Total de Prospects',\n",
    "                (SELECT COUNT(*) FROM prospects)\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Candidatos Contratados',\n",
    "                (SELECT COUNT(*) FROM prospects WHERE contratado = 1)\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Vagas com Prospects',\n",
    "                (SELECT COUNT(DISTINCT id_vaga) FROM prospects)\n",
    "            \"\"\"\n",
    "            \n",
    "            stats_df = pd.read_sql(stats_query, conn)\n",
    "            \n",
    "            for _, row in stats_df.iterrows():\n",
    "                print(f\"üìä {row['metrica']}: {row['valor']:,}\")\n",
    "        \n",
    "        # Top 5 vagas com mais candidatos\n",
    "        print(f\"\\n=== TOP 5 VAGAS COM MAIS CANDIDATOS ===\")\n",
    "        with engine.connect() as conn:\n",
    "            top_vagas_query = '''\n",
    "            SELECT \n",
    "                v.titulo_vaga,\n",
    "                v.cliente,\n",
    "                COUNT(p.codigo_candidato) as total_candidatos\n",
    "            FROM vagas v\n",
    "            LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "            GROUP BY v.id_vaga, v.titulo_vaga, v.cliente\n",
    "            ORDER BY total_candidatos DESC\n",
    "            LIMIT 5\n",
    "            '''\n",
    "            \n",
    "            top_vagas_df = pd.read_sql(top_vagas_query, conn)\n",
    "            \n",
    "            for _, row in top_vagas_df.iterrows():\n",
    "                titulo = row['titulo_vaga'][:40]\n",
    "                cliente = row['cliente'][:20]\n",
    "                total = row['total_candidatos']\n",
    "                print(f\"   {titulo:<40} | {cliente:<20} | {total:,} candidatos\")\n",
    "        \n",
    "        # Distribui√ß√£o por n√≠vel profissional\n",
    "        print(f\"\\n=== DISTRIBUI√á√ÉO POR N√çVEL PROFISSIONAL ===\")\n",
    "        with engine.connect() as conn:\n",
    "            nivel_query = '''\n",
    "            SELECT \n",
    "                nivel_profissional, \n",
    "                COUNT(*) as total\n",
    "            FROM vagas\n",
    "            WHERE nivel_profissional != '' AND nivel_profissional IS NOT NULL\n",
    "            GROUP BY nivel_profissional\n",
    "            ORDER BY total DESC\n",
    "            '''\n",
    "            \n",
    "            nivel_df = pd.read_sql(nivel_query, conn)\n",
    "            \n",
    "            for _, row in nivel_df.iterrows():\n",
    "                print(f\"   {row['nivel_profissional']:<20} | {row['total']:,} vagas\")\n",
    "        \n",
    "        # Taxa de contrata√ß√£o por √°rea (usando HAVING otimizado para PostgreSQL)\n",
    "        print(f\"\\n=== TAXA DE CONTRATA√á√ÉO POR √ÅREA ===\")\n",
    "        with engine.connect() as conn:\n",
    "            area_query = '''\n",
    "            SELECT \n",
    "                v.areas_atuacao,\n",
    "                COUNT(p.codigo_candidato) as total_candidatos,\n",
    "                SUM(p.contratado) as contratados,\n",
    "                ROUND(AVG(p.contratado::numeric) * 100, 2) as taxa_contratacao\n",
    "            FROM vagas v\n",
    "            LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "            WHERE v.areas_atuacao != '' AND v.areas_atuacao IS NOT NULL\n",
    "            GROUP BY v.areas_atuacao\n",
    "            HAVING COUNT(p.codigo_candidato) >= 50\n",
    "            ORDER BY taxa_contratacao DESC\n",
    "            LIMIT 10\n",
    "            '''\n",
    "            \n",
    "            area_df = pd.read_sql(area_query, conn)\n",
    "            \n",
    "            for _, row in area_df.iterrows():\n",
    "                area = row['areas_atuacao'][:30]\n",
    "                total_cand = row['total_candidatos']\n",
    "                contratados = row['contratados']\n",
    "                taxa = row['taxa_contratacao']\n",
    "                print(f\"   {area:<30} | {total_cand:,} candidatos | {contratados} contratados | {taxa}%\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao consultar estat√≠sticas PostgreSQL: {e}\")\n",
    "\n",
    "def validate_postgresql_consistency():\n",
    "    \"\"\"Valida a consist√™ncia dos dados no PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        print(\"\\n=== VALIDA√á√ÉO DE CONSIST√äNCIA POSTGRESQL ===\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            # Verificar chaves estrangeiras √≥rf√£s\n",
    "            orphan_query = '''\n",
    "            SELECT COUNT(*) as orphans FROM prospects p\n",
    "            WHERE p.id_vaga NOT IN (SELECT id_vaga FROM vagas)\n",
    "            OR p.codigo_candidato NOT IN (SELECT codigo_candidato FROM candidatos)\n",
    "            '''\n",
    "            \n",
    "            orphan_df = pd.read_sql(orphan_query, conn)\n",
    "            orphans = orphan_df['orphans'].iloc[0]\n",
    "            \n",
    "            if orphans == 0:\n",
    "                print(\"‚úÖ Todas as chaves estrangeiras est√£o consistentes\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {orphans} prospects com chaves estrangeiras inv√°lidas\")\n",
    "            \n",
    "            # Verificar duplicatas usando PostgreSQL espec√≠fico\n",
    "            dup_query = '''\n",
    "            SELECT \n",
    "                'vagas' as tabela,\n",
    "                COUNT(*) - COUNT(DISTINCT id_vaga) as duplicatas\n",
    "            FROM vagas\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'candidatos',\n",
    "                COUNT(*) - COUNT(DISTINCT codigo_candidato)\n",
    "            FROM candidatos\n",
    "            '''\n",
    "            \n",
    "            dup_df = pd.read_sql(dup_query, conn)\n",
    "            total_dups = dup_df['duplicatas'].sum()\n",
    "            \n",
    "            if total_dups == 0:\n",
    "                print(\"‚úÖ N√£o h√° duplicatas nas chaves prim√°rias\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {total_dups} duplicatas encontradas\")\n",
    "                for _, row in dup_df.iterrows():\n",
    "                    if row['duplicatas'] > 0:\n",
    "                        print(f\"    {row['tabela']}: {row['duplicatas']} duplicatas\")\n",
    "            \n",
    "            # Verificar valores nulos em campos cr√≠ticos\n",
    "            null_checks = {\n",
    "                'vagas.titulo_vaga': \"SELECT COUNT(*) FROM vagas WHERE titulo_vaga IS NULL OR titulo_vaga = ''\",\n",
    "                'candidatos.nome': \"SELECT COUNT(*) FROM candidatos WHERE nome IS NULL OR nome = ''\",\n",
    "                'prospects.situacao_candidado': \"SELECT COUNT(*) FROM prospects WHERE situacao_candidado IS NULL OR situacao_candidado = ''\"\n",
    "            }\n",
    "            \n",
    "            all_good = True\n",
    "            for campo, query in null_checks.items():\n",
    "                null_df = pd.read_sql(query, conn)\n",
    "                nulls = null_df.iloc[0, 0]\n",
    "                if nulls == 0:\n",
    "                    print(f\"‚úÖ {campo}: Sem valores nulos/vazios\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  {campo}: {nulls} registros com valores nulos/vazios\")\n",
    "                    all_good = False\n",
    "            \n",
    "            # Verificar √≠ndices PostgreSQL\n",
    "            indexes_query = '''\n",
    "            SELECT \n",
    "                indexname,\n",
    "                tablename,\n",
    "                indexdef\n",
    "            FROM pg_indexes \n",
    "            WHERE schemaname = 'public' \n",
    "            AND tablename IN ('vagas', 'candidatos', 'prospects', 'predicoes')\n",
    "            ORDER BY tablename, indexname\n",
    "            '''\n",
    "            \n",
    "            indexes_df = pd.read_sql(indexes_query, conn)\n",
    "            print(f\"\\nüìã √çNDICES POSTGRESQL: {len(indexes_df)} √≠ndices criados\")\n",
    "            \n",
    "            # Mostrar √≠ndices GIN (espec√≠ficos do PostgreSQL)\n",
    "            gin_indexes = indexes_df[indexes_df['indexdef'].str.contains('gin', case=False)]\n",
    "            if len(gin_indexes) > 0:\n",
    "                print(\"üîç √çndices GIN para busca full-text:\")\n",
    "                for _, idx in gin_indexes.iterrows():\n",
    "                    print(f\"   {idx['tablename']}.{idx['indexname']}\")\n",
    "            \n",
    "            if all_good:\n",
    "                print(\"\\nüéâ Banco PostgreSQL est√° consistente e pronto para uso!\")\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è  Algumas inconsist√™ncias encontradas - revisar dados se necess√°rio\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na valida√ß√£o PostgreSQL: {e}\")\n",
    "\n",
    "def test_postgresql_advanced_features():\n",
    "    \"\"\"Testa recursos avan√ßados do PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        print(\"\\n=== TESTANDO RECURSOS AVAN√áADOS POSTGRESQL ===\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            # Teste de busca full-text com GIN\n",
    "            fulltext_query = '''\n",
    "            SELECT \n",
    "                titulo_vaga,\n",
    "                areas_atuacao,\n",
    "                ts_rank(to_tsvector('portuguese', areas_atuacao), \n",
    "                       to_tsquery('portuguese', 'desenvolvimento')) as rank\n",
    "            FROM vagas \n",
    "            WHERE to_tsvector('portuguese', areas_atuacao) @@ to_tsquery('portuguese', 'desenvolvimento')\n",
    "            ORDER BY rank DESC\n",
    "            LIMIT 5\n",
    "            '''\n",
    "            \n",
    "            try:\n",
    "                fulltext_df = pd.read_sql(fulltext_query, conn)\n",
    "                print(f\"‚úÖ Busca full-text funcionando: {len(fulltext_df)} resultados para 'desenvolvimento'\")\n",
    "                if len(fulltext_df) > 0:\n",
    "                    print(\"   Top resultado:\", fulltext_df.iloc[0]['titulo_vaga'][:50])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Busca full-text: {e}\")\n",
    "            \n",
    "            # Teste das views criadas\n",
    "            view_test_query = \"SELECT COUNT(*) as total FROM vw_vagas_stats WHERE total_candidatos > 0\"\n",
    "            try:\n",
    "                view_df = pd.read_sql(view_test_query, conn)\n",
    "                print(f\"‚úÖ View vw_vagas_stats funcionando: {view_df['total'].iloc[0]} vagas com candidatos\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  View test: {e}\")\n",
    "            \n",
    "            # Informa√ß√µes do banco PostgreSQL\n",
    "            db_info_query = '''\n",
    "            SELECT \n",
    "                current_database() as database_name,\n",
    "                version() as postgresql_version,\n",
    "                current_user as current_user,\n",
    "                inet_server_addr() as server_ip,\n",
    "                inet_server_port() as server_port\n",
    "            '''\n",
    "            \n",
    "            db_info_df = pd.read_sql(db_info_query, conn)\n",
    "            print(f\"\\nüóÑÔ∏è  INFORMA√á√ïES DO BANCO:\")\n",
    "            print(f\"   Database: {db_info_df['database_name'].iloc[0]}\")\n",
    "            print(f\"   PostgreSQL: {db_info_df['postgresql_version'].iloc[0].split(' on ')[0]}\")\n",
    "            print(f\"   Usu√°rio: {db_info_df['current_user'].iloc[0]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro nos testes avan√ßados: {e}\")\n",
    "\n",
    "# Executar todas as consultas e valida√ß√µes PostgreSQL\n",
    "query_postgresql_stats()\n",
    "validate_postgresql_consistency()\n",
    "test_postgresql_advanced_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53310f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURA√á√ÉO DA APLICA√á√ÉO FLASK PARA POSTGRESQL\n",
    "def generate_flask_postgresql_config():\n",
    "    \"\"\"Gera configura√ß√£o para a aplica√ß√£o Flask usar PostgreSQL\"\"\"\n",
    "    \n",
    "    # C√≥digo de configura√ß√£o para app.py\n",
    "    flask_config_code = '''\n",
    "# Configura√ß√£o PostgreSQL para app.py\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "# Configura√ß√µes do PostgreSQL\n",
    "PG_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'otimizador_entrevistas'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432))\n",
    "}\n",
    "\n",
    "def create_pg_engine():\n",
    "    \"\"\"Cria engine SQLAlchemy para PostgreSQL\"\"\"\n",
    "    connection_string = f\"postgresql://{PG_CONFIG['user']}:{PG_CONFIG['password']}@{PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\"\n",
    "    return create_engine(connection_string, pool_pre_ping=True, pool_recycle=300)\n",
    "\n",
    "def load_vagas_from_postgres():\n",
    "    \"\"\"Carrega vagas do PostgreSQL com filtro de prospects\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT v.* \n",
    "    FROM vagas v \n",
    "    INNER JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "    ORDER BY v.titulo_vaga\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "def load_candidates_from_postgres():\n",
    "    \"\"\"Carrega candidatos do PostgreSQL\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    return pd.read_sql(\"SELECT * FROM candidatos ORDER BY nome\", engine)\n",
    "\n",
    "def load_prospects_from_postgres():\n",
    "    \"\"\"Carrega prospects do PostgreSQL\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    return pd.read_sql(\"SELECT * FROM prospects ORDER BY created_at DESC\", engine)\n",
    "\n",
    "def get_vagas_com_prospects():\n",
    "    \"\"\"Retorna set de vagas que t√™m prospects\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    query = \"SELECT DISTINCT id_vaga FROM prospects\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return set(df['id_vaga'].tolist())\n",
    "\n",
    "def save_predicao_to_postgres(predicao_data):\n",
    "    \"\"\"Salva predi√ß√£o no PostgreSQL\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    df = pd.DataFrame([predicao_data])\n",
    "    df.to_sql('predicoes', engine, if_exists='append', index=False)\n",
    "    print(f\"‚úÖ Predi√ß√£o salva no PostgreSQL para vaga {predicao_data.get('id_vaga')}\")\n",
    "\n",
    "def get_vaga_candidatos_postgres(id_vaga, limit=50):\n",
    "    \"\"\"Busca candidatos para uma vaga espec√≠fica com pagina√ß√£o\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    query = \"\"\"\n",
    "    SELECT c.*, p.situacao_candidado, p.contratado\n",
    "    FROM candidatos c\n",
    "    INNER JOIN prospects p ON c.codigo_candidato = p.codigo_candidato\n",
    "    WHERE p.id_vaga = %(id_vaga)s\n",
    "    ORDER BY p.contratado DESC, c.nome\n",
    "    LIMIT %(limit)s\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine, params={'id_vaga': id_vaga, 'limit': limit})\n",
    "\n",
    "# Fun√ß√£o para busca otimizada com PostgreSQL\n",
    "def search_vagas_postgres(search_term=None, nivel_filter=None, page=1, per_page=10):\n",
    "    \"\"\"Busca vagas com filtros e pagina√ß√£o otimizada\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    \n",
    "    base_query = \"\"\"\n",
    "    SELECT v.*, COUNT(p.codigo_candidato) as total_candidatos\n",
    "    FROM vagas v\n",
    "    LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "    WHERE 1=1\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    if search_term:\n",
    "        base_query += \" AND (v.titulo_vaga ILIKE %(search)s OR v.cliente ILIKE %(search)s)\"\n",
    "        params['search'] = f'%{search_term}%'\n",
    "    \n",
    "    if nivel_filter:\n",
    "        base_query += \" AND v.nivel_profissional = %(nivel)s\"\n",
    "        params['nivel'] = nivel_filter\n",
    "    \n",
    "    base_query += \" GROUP BY v.id_vaga\"\n",
    "    base_query += \" HAVING COUNT(p.codigo_candidato) > 0\"\n",
    "    base_query += \" ORDER BY total_candidatos DESC\"\n",
    "    \n",
    "    # Pagina√ß√£o\n",
    "    offset = (page - 1) * per_page\n",
    "    paginated_query = base_query + f\" LIMIT {per_page} OFFSET {offset}\"\n",
    "    \n",
    "    return pd.read_sql(paginated_query, engine, params=params)\n",
    "'''\n",
    "    \n",
    "    # Salvar configura√ß√£o\n",
    "    config_file_path = Path(\"../app/postgresql_integration.py\")\n",
    "    with open(config_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(flask_config_code)\n",
    "    \n",
    "    print(f\"\udc0d Configura√ß√£o Flask-PostgreSQL gerada: {config_file_path}\")\n",
    "    \n",
    "    # Arquivo .env de exemplo\n",
    "    env_example = '''# Configura√ß√µes PostgreSQL para .env\n",
    "DB_HOST=localhost\n",
    "DB_NAME=otimizador_entrevistas\n",
    "DB_USER=postgres\n",
    "DB_PASSWORD=sua_senha_aqui\n",
    "DB_PORT=5432\n",
    "\n",
    "# Para produ√ß√£o, use valores seguros:\n",
    "# DB_HOST=seu_servidor_postgres\n",
    "# DB_PASSWORD=senha_forte_aleatoria\n",
    "'''\n",
    "    \n",
    "    env_file_path = Path(\"../app/.env.example\")\n",
    "    with open(env_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(env_example)\n",
    "    \n",
    "    print(f\"üìÑ Arquivo .env.example criado: {env_file_path}\")\n",
    "    \n",
    "    # Docker Compose para PostgreSQL\n",
    "    docker_compose = '''version: '3.8'\n",
    "\n",
    "services:\n",
    "  postgresql:\n",
    "    image: postgres:15-alpine\n",
    "    container_name: otimizador_postgres\n",
    "    environment:\n",
    "      POSTGRES_DB: otimizador_entrevistas\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: postgres\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n",
    "    restart: unless-stopped\n",
    "\n",
    "  app:\n",
    "    build: .\n",
    "    container_name: otimizador_app\n",
    "    depends_on:\n",
    "      - postgresql\n",
    "    environment:\n",
    "      DB_HOST: postgresql\n",
    "      DB_NAME: otimizador_entrevistas\n",
    "      DB_USER: postgres\n",
    "      DB_PASSWORD: postgres\n",
    "      DB_PORT: 5432\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "'''\n",
    "    \n",
    "    docker_file_path = Path(\"../docker-compose.postgres.yml\")\n",
    "    with open(docker_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(docker_compose)\n",
    "    \n",
    "    print(f\"\udc33 Docker Compose PostgreSQL criado: {docker_file_path}\")\n",
    "    \n",
    "    print(f\"\\nüìã PR√ìXIMOS PASSOS PARA INTEGRA√á√ÉO:\")\n",
    "    print(\"1. Instale depend√™ncias: pip install psycopg2-binary sqlalchemy\")\n",
    "    print(\"2. Configure .env: cp app/.env.example app/.env\")\n",
    "    print(\"3. Inicie PostgreSQL: docker-compose -f docker-compose.postgres.yml up -d postgresql\")\n",
    "    print(\"4. Execute este notebook para migrar os dados\")\n",
    "    print(\"5. Integre postgresql_integration.py no app.py\")\n",
    "    print(\"6. Teste a aplica√ß√£o: python app/app.py\")\n",
    "\n",
    "# Gerar configura√ß√µes Flask-PostgreSQL\n",
    "print(\"\\n=== GERANDO CONFIGURA√á√ïES FLASK-POSTGRESQL ===\")\n",
    "generate_flask_postgresql_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5489f",
   "metadata": {},
   "source": [
    "## 4. Engenharia de Features\n",
    "\n",
    "Vamos criar features relevantes para o modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d710a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando features...\n",
      "Features categ√≥ricas dispon√≠veis: ['nivel_profissional', 'nivel_academico_vaga', 'nivel_ingles_vaga', 'areas_atuacao', 'tipo_contratacao', 'nivel_academico_candidato', 'nivel_ingles_candidato', 'area_de_atuacao']\n",
      "Tamanho do dataset: (45071, 32)\n",
      "\n",
      "=== Distribui√ß√£o de features categ√≥ricas ===\n",
      "\n",
      "nivel_profissional:\n",
      "nivel_profissional\n",
      "s√™nior          17987\n",
      "analista        14684\n",
      "pleno            8597\n",
      "j√∫nior           1465\n",
      "especialista      912\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nivel_academico_vaga:\n",
      "nivel_academico_vaga\n",
      "ensino superior completo      33830\n",
      "ensino m√©dio completo          5391\n",
      "ensino t√©cnico completo        3501\n",
      "ensino superior cursando       1441\n",
      "ensino superior incompleto      520\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nivel_ingles_vaga:\n",
      "nivel_ingles_vaga\n",
      "b√°sico           14913\n",
      "nenhum           11230\n",
      "avan√ßado          8736\n",
      "fluente           4933\n",
      "intermedi√°rio     4561\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preparar features b√°sicas para o modelo\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepara as features b√°sicas para o modelo\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Tratar valores nulos e padronizar\n",
    "    df_features = df_features.fillna('')\n",
    "    \n",
    "    # Criar feature de texto combinada (compet√™ncias + conhecimentos)\n",
    "    df_features['competencias_combinadas'] = (\n",
    "        df_features['competencias_tecnicas_requeridas'].astype(str) + ' ' +\n",
    "        df_features['conhecimentos_tecnicos'].astype(str) + ' ' +\n",
    "        df_features['principais_atividades'].astype(str)\n",
    "    ).str.lower().str.strip()\n",
    "    \n",
    "    # Verificar colunas dispon√≠veis ap√≥s merge\n",
    "    print(f\"Colunas dispon√≠veis no DataFrame: {list(df_features.columns)}\")\n",
    "    \n",
    "    # Definir features categ√≥ricas baseadas nas colunas que realmente existem\n",
    "    potential_categorical = [\n",
    "        'nivel_profissional', \n",
    "        'nivel_academico',  # Pode ser da vaga ou candidato\n",
    "        'nivel_ingles',     # Pode ser da vaga ou candidato  \n",
    "        'areas_atuacao', \n",
    "        'tipo_contratacao', \n",
    "        'area_de_atuacao'\n",
    "    ]\n",
    "    \n",
    "    # Verificar quais colunas realmente existem\n",
    "    available_categorical = []\n",
    "    for feature in potential_categorical:\n",
    "        if feature in df_features.columns:\n",
    "            available_categorical.append(feature)\n",
    "            # Limpar e padronizar\n",
    "            df_features[feature] = df_features[feature].astype(str).str.strip().str.lower()\n",
    "            df_features[feature] = df_features[feature].replace('', 'n√£o_informado')\n",
    "    \n",
    "    print(f\"Features categ√≥ricas identificadas: {available_categorical}\")\n",
    "    \n",
    "    return df_features, available_categorical\n",
    "\n",
    "# Preparar features b√°sicas\n",
    "print(\"Preparando features b√°sicas...\")\n",
    "df_features, categorical_cols = prepare_features(df_final)\n",
    "\n",
    "print(f\"Features categ√≥ricas dispon√≠veis: {categorical_cols}\")\n",
    "print(f\"Tamanho do dataset: {df_features.shape}\")\n",
    "\n",
    "# Verificar distribui√ß√£o de algumas features categ√≥ricas\n",
    "print(\"\\n=== Distribui√ß√£o de features categ√≥ricas ===\")\n",
    "for col in categorical_cols[:3]:  # Mostrar s√≥ as primeiras 3\n",
    "    if col in df_features.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df_features[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404462b",
   "metadata": {},
   "source": [
    "### 4.2. Features Avan√ßadas\n",
    "\n",
    "Agora vamos criar features inteligentes que capturam a compatibilidade entre vagas e candidatos:\n",
    "\n",
    "1. **Tech Match Score**: Compatibilidade t√©cnica baseada em sobreposi√ß√£o de palavras-chave\n",
    "2. **Academic Match**: Compatibilidade de n√≠vel acad√™mico usando hierarquia\n",
    "3. **English Match**: Compatibilidade de n√≠vel de ingl√™s\n",
    "4. **Combined Text**: Texto unificado para an√°lise sem√¢ntica com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIA√á√ÉO DE FEATURES AVAN√áADAS\n",
    "print(\"=== CRIANDO FEATURES AVAN√áADAS ===\")\n",
    "print(\"Baseado na an√°lise dos dados, vamos criar:\")\n",
    "print(\"- Compet√™ncias t√©cnicas requeridas vs Conhecimentos t√©cnicos\")\n",
    "print(\"- N√≠vel acad√™mico (vaga vs candidato)\")\n",
    "print(\"- N√≠vel de idiomas\")\n",
    "print(\"- Principais atividades\")\n",
    "print(\"- Features categ√≥ricas originais\")\n",
    "\n",
    "def create_enhanced_features(df):\n",
    "    \"\"\"Cria features aprimoradas usando todas as informa√ß√µes dispon√≠veis\"\"\"\n",
    "    \n",
    "    # Verificar colunas dispon√≠veis\n",
    "    print(f\"Colunas dispon√≠veis no DataFrame: {list(df.columns)}\")\n",
    "    \n",
    "    # 1. Match de compet√™ncias t√©cnicas\n",
    "    def calculate_tech_match(row):\n",
    "        \"\"\"Calcula match entre compet√™ncias requeridas e conhecimentos t√©cnicos\"\"\"\n",
    "        competencias_req = str(row.get('competencias_tecnicas_requeridas', '')).lower()\n",
    "        conhecimentos = str(row.get('conhecimentos_tecnicos', '')).lower()\n",
    "        \n",
    "        if not competencias_req or competencias_req == 'nan':\n",
    "            return 0.0\n",
    "        if not conhecimentos or conhecimentos == 'nan':\n",
    "            return 0.0\n",
    "            \n",
    "        # Palavras-chave t√©cnicas\n",
    "        comp_words = set(competencias_req.split())\n",
    "        conhec_words = set(conhecimentos.split())\n",
    "        \n",
    "        if len(comp_words) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Interse√ß√£o / uni√£o\n",
    "        intersection = comp_words.intersection(conhec_words)\n",
    "        match_score = len(intersection) / len(comp_words)\n",
    "        \n",
    "        return round(match_score, 2)\n",
    "    \n",
    "    # 2. Match de n√≠vel acad√™mico\n",
    "    def calculate_academic_match(row):\n",
    "        \"\"\"Verifica compatibilidade do n√≠vel acad√™mico\"\"\"\n",
    "        # Usar nomes corretos das colunas ap√≥s merge\n",
    "        nivel_vaga = str(row.get('nivel_academico', '')).lower()  # Da vaga\n",
    "        nivel_candidato = str(row.get('nivel_academico_candidato', \n",
    "                                   row.get('nivel_academico', ''))).lower()  # Do candidato\n",
    "        \n",
    "        # Hierarquia acad√™mica\n",
    "        hierarchy = {\n",
    "            'ensino m√©dio': 1,\n",
    "            't√©cnico': 2,\n",
    "            'tecn√≥logo': 3,\n",
    "            'superior': 4,\n",
    "            'p√≥s-gradua√ß√£o': 5,\n",
    "            'mestrado': 6,\n",
    "            'doutorado': 7\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(nivel_vaga, 0)\n",
    "        candidato_level = hierarchy.get(nivel_candidato, 0)\n",
    "        \n",
    "        if vaga_level == 0 or candidato_level == 0:\n",
    "            return 'indefinido'\n",
    "        elif candidato_level >= vaga_level:\n",
    "            return 'compat√≠vel'\n",
    "        else:\n",
    "            return 'insuficiente'\n",
    "    \n",
    "    # 3. Match de ingl√™s\n",
    "    def calculate_english_match(row):\n",
    "        \"\"\"Verifica compatibilidade do n√≠vel de ingl√™s\"\"\"\n",
    "        # Usar nomes corretos das colunas ap√≥s merge\n",
    "        nivel_vaga = str(row.get('nivel_ingles', '')).lower()  # Da vaga\n",
    "        nivel_candidato = str(row.get('nivel_ingles_candidato',\n",
    "                                   row.get('nivel_ingles', ''))).lower()  # Do candidato\n",
    "        \n",
    "        # Hierarquia de ingl√™s\n",
    "        hierarchy = {\n",
    "            'nenhum': 0,\n",
    "            'b√°sico': 1,\n",
    "            'intermedi√°rio': 2,\n",
    "            'avan√ßado': 3,\n",
    "            'fluente': 4,\n",
    "            'nativo': 5\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(nivel_vaga, -1)\n",
    "        candidato_level = hierarchy.get(nivel_candidato, -1)\n",
    "        \n",
    "        if vaga_level == -1 or candidato_level == -1:\n",
    "            return 'indefinido'\n",
    "        elif candidato_level >= vaga_level:\n",
    "            return 'compat√≠vel'\n",
    "        else:\n",
    "            return 'insuficiente'\n",
    "    \n",
    "    # Aplicar as fun√ß√µes\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    print(\"Calculando match de compet√™ncias t√©cnicas...\")\n",
    "    df_enhanced['tech_match_score'] = df.apply(calculate_tech_match, axis=1)\n",
    "    \n",
    "    print(\"Calculando compatibilidade acad√™mica...\")\n",
    "    df_enhanced['academic_match'] = df.apply(calculate_academic_match, axis=1)\n",
    "    \n",
    "    print(\"Calculando compatibilidade de ingl√™s...\")\n",
    "    df_enhanced['english_match'] = df.apply(calculate_english_match, axis=1)\n",
    "    \n",
    "    # 4. Criar texto combinado para an√°lise sem√¢ntica\n",
    "    def create_combined_text(row):\n",
    "        \"\"\"Combina textos relevantes para an√°lise\"\"\"\n",
    "        parts = [\n",
    "            str(row.get('competencias_tecnicas_requeridas', '')),\n",
    "            str(row.get('conhecimentos_tecnicos', '')),\n",
    "            str(row.get('principais_atividades', '')),\n",
    "            str(row.get('area_de_atuacao', ''))\n",
    "        ]\n",
    "        return ' '.join([p for p in parts if p and p != 'nan']).lower().strip()\n",
    "    \n",
    "    df_enhanced['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Aplicar melhorias no dataset preparado\n",
    "df_enhanced = create_enhanced_features(df_features)\n",
    "\n",
    "print(f\"\\nDataset aprimorado: {df_enhanced.shape}\")\n",
    "print(\"\\nNovas features criadas:\")\n",
    "print(\"- tech_match_score: Score de match t√©cnico (0.0 a 1.0)\")\n",
    "print(\"- academic_match: Compatibilidade acad√™mica\")\n",
    "print(\"- english_match: Compatibilidade de ingl√™s\")\n",
    "print(\"- combined_text: Texto combinado para an√°lise sem√¢ntica\")\n",
    "\n",
    "# Verificar as novas features\n",
    "print(f\"\\n=== ESTAT√çSTICAS DAS NOVAS FEATURES ===\")\n",
    "print(f\"Tech match score - M√©dia: {df_enhanced['tech_match_score'].mean():.2f}\")\n",
    "print(f\"Tech match score - Range: {df_enhanced['tech_match_score'].min():.2f} a {df_enhanced['tech_match_score'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o Academic Match:\")\n",
    "print(df_enhanced['academic_match'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o English Match:\")\n",
    "print(df_enhanced['english_match'].value_counts())\n",
    "\n",
    "print(f\"\\nAmostra dos dados aprimorados:\")\n",
    "sample_cols = ['nome_candidato', 'titulo_vaga', 'tech_match_score', 'academic_match', 'english_match', 'contratado']\n",
    "available_cols = [col for col in sample_cols if col in df_enhanced.columns]\n",
    "if available_cols:\n",
    "    print(df_enhanced[available_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e255d8",
   "metadata": {},
   "source": [
    "## 5. Pipeline de Machine Learning\n",
    "\n",
    "Agora vamos criar um pipeline completo que combina:\n",
    "- **Pr√©-processamento autom√°tico** de features num√©ricas, categ√≥ricas e texto\n",
    "- **Modelo Random Forest** otimizado para o problema de classifica√ß√£o\n",
    "- **Valida√ß√£o cruzada** para avaliar a performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIANDO PIPELINE DE MACHINE LEARNING COMPLETO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CRIANDO PIPELINE COMPLETO ===\")\n",
    "\n",
    "# Selecionar features para o modelo\n",
    "def select_features_for_model(df):\n",
    "    \"\"\"Seleciona features para o modelo\"\"\"\n",
    "    \n",
    "    # Features num√©ricas\n",
    "    numeric_features = ['tech_match_score']\n",
    "    \n",
    "    # Features categ√≥ricas\n",
    "    categorical_features = [\n",
    "        'nivel_profissional',\n",
    "        'areas_atuacao', \n",
    "        'area_de_atuacao',\n",
    "        'academic_match',\n",
    "        'english_match'\n",
    "    ]\n",
    "    \n",
    "    # Feature de texto\n",
    "    text_features = ['combined_text']\n",
    "    \n",
    "    # Verificar quais features existem no dataframe\n",
    "    available_numeric = [f for f in numeric_features if f in df.columns]\n",
    "    available_categorical = [f for f in categorical_features if f in df.columns]\n",
    "    available_text = [f for f in text_features if f in df.columns]\n",
    "    \n",
    "    print(f\"Features num√©ricas dispon√≠veis: {available_numeric}\")\n",
    "    print(f\"Features categ√≥ricas dispon√≠veis: {available_categorical}\")\n",
    "    print(f\"Features de texto dispon√≠veis: {available_text}\")\n",
    "    \n",
    "    return available_numeric, available_categorical, available_text\n",
    "\n",
    "# Preparar dados para treinamento\n",
    "numeric_cols, categorical_cols, text_cols = select_features_for_model(df_enhanced)\n",
    "\n",
    "# Criar features X e target y\n",
    "all_feature_cols = numeric_cols + categorical_cols + text_cols\n",
    "X_enhanced = df_enhanced[all_feature_cols].copy()\n",
    "y_enhanced = df_enhanced['contratado'].copy()\n",
    "\n",
    "print(f\"\\nDataset para treinamento: {X_enhanced.shape}\")\n",
    "print(f\"Distribui√ß√£o do target: {y_enhanced.value_counts().to_dict()}\")\n",
    "\n",
    "# Criar transformadores\n",
    "print(\"\\nCriando transformadores...\")\n",
    "\n",
    "# Para features num√©ricas\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Para features categ√≥ricas\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# Para features de texto\n",
    "text_transformer = TfidfVectorizer(\n",
    "    max_features=50,  # Limitado para dataset pequeno\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,  # Aceitar termos que aparecem pelo menos 1 vez\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Criar ColumnTransformer\n",
    "transformers = []\n",
    "\n",
    "if numeric_cols:\n",
    "    transformers.append(('num', numeric_transformer, numeric_cols))\n",
    "if categorical_cols:\n",
    "    transformers.append(('cat', categorical_transformer, categorical_cols))\n",
    "if text_cols:\n",
    "    transformers.append(('text', text_transformer, text_cols[0]))  # TfidfVectorizer espera uma string\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError(\"Nenhuma feature v√°lida encontrada!\")\n",
    "\n",
    "preprocessor_enhanced = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Criar pipeline completo\n",
    "pipeline_enhanced = Pipeline([\n",
    "    ('preprocessor', preprocessor_enhanced),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=20,  # N√∫mero de √°rvores\n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Para lidar com desbalanceamento\n",
    "        max_depth=5,  # Profundidade das √°rvores\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(f\"Pipeline criado com {len(transformers)} tipos de transformadores!\")\n",
    "\n",
    "# Treinar modelo\n",
    "print(\"\\nTreinando modelo...\")\n",
    "try:\n",
    "    pipeline_enhanced.fit(X_enhanced, y_enhanced)\n",
    "    print(\"‚úÖ Modelo treinado com sucesso!\")\n",
    "    \n",
    "    # Fazer predi√ß√µes no conjunto de treino\n",
    "    predictions_enhanced = pipeline_enhanced.predict(X_enhanced)\n",
    "    probabilities_enhanced = pipeline_enhanced.predict_proba(X_enhanced)\n",
    "    \n",
    "    # Avaliar desempenho\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    accuracy_enhanced = accuracy_score(y_enhanced, predictions_enhanced)\n",
    "    print(f\"\\nAcur√°cia do modelo: {accuracy_enhanced:.3f}\")\n",
    "    \n",
    "    print(\"\\nRelat√≥rio de classifica√ß√£o:\")\n",
    "    print(classification_report(y_enhanced, predictions_enhanced))\n",
    "    \n",
    "    print(\"\\nMatriz de confus√£o:\")\n",
    "    print(confusion_matrix(y_enhanced, predictions_enhanced))\n",
    "    \n",
    "    # Valida√ß√£o cruzada (se poss√≠vel)\n",
    "    if len(X_enhanced) >= 3:\n",
    "        try:\n",
    "            cv_scores = cross_val_score(pipeline_enhanced, X_enhanced, y_enhanced, cv=3, scoring='accuracy')\n",
    "            print(f\"\\nValida√ß√£o cruzada (CV=3): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        except Exception as cv_error:\n",
    "            print(f\"\\nValida√ß√£o cruzada n√£o poss√≠vel: {cv_error}\")\n",
    "    \n",
    "    # Mostrar import√¢ncia das features\n",
    "    print(f\"\\n=== AN√ÅLISE DAS FEATURES ===\")\n",
    "    try:\n",
    "        feature_importance = pipeline_enhanced.named_steps['classifier'].feature_importances_\n",
    "        print(f\"N√∫mero de features ap√≥s transforma√ß√£o: {len(feature_importance)}\")\n",
    "        \n",
    "        # Features mais importantes\n",
    "        if len(feature_importance) > 0:\n",
    "            top_indices = np.argsort(feature_importance)[-10:][::-1]\n",
    "            print(\"\\nTop 10 features mais importantes:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1:2d}. Feature_{idx:<10} {feature_importance[idx]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"N√£o foi poss√≠vel calcular import√¢ncia das features: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no treinamento do modelo: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c047c8",
   "metadata": {},
   "source": [
    "## 6. Serializa√ß√£o e Teste do Modelo\n",
    "\n",
    "Agora vamos salvar o modelo treinado para uso em produ√ß√£o e test√°-lo com exemplos reais para validar seu funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVAR MODELO E CRIAR FUN√á√ÉO DE TESTE\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"=== SALVANDO MODELO PARA PRODU√á√ÉO ===\")\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "os.makedirs('../app/models', exist_ok=True)\n",
    "\n",
    "# Salvar pipeline treinado\n",
    "model_path = '../app/models/pipeline_aprimorado.joblib'\n",
    "joblib.dump(pipeline_enhanced, model_path)\n",
    "print(f\"‚úÖ Modelo salvo em: {model_path}\")\n",
    "\n",
    "# Salvar metadados do modelo\n",
    "model_metadata = {\n",
    "    'model_version': '2.0_enhanced',\n",
    "    'features': {\n",
    "        'numeric': numeric_cols,\n",
    "        'categorical': categorical_cols,\n",
    "        'text': text_cols\n",
    "    },\n",
    "    'total_features_after_transform': len(pipeline_enhanced.named_steps['classifier'].feature_importances_),\n",
    "    'model_type': 'RandomForestClassifier_Enhanced',\n",
    "    'training_date': str(pd.Timestamp.now()),\n",
    "    'dataset_size': len(X_enhanced),\n",
    "    'accuracy': accuracy_enhanced,\n",
    "    'class_distribution': y_enhanced.value_counts().to_dict(),\n",
    "    'feature_engineering': [\n",
    "        'tech_match_score: Score de compatibilidade t√©cnica (0.0-1.0)',\n",
    "        'academic_match: Compatibilidade acad√™mica (compat√≠vel/insuficiente/indefinido)',\n",
    "        'english_match: Compatibilidade de ingl√™s (compat√≠vel/insuficiente/indefinido)',\n",
    "        'combined_text: An√°lise sem√¢ntica de compet√™ncias e atividades'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_path = '../app/models/model_metadata_enhanced.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Metadados salvos em: {metadata_path}\")\n",
    "\n",
    "# FUN√á√ÉO DE TESTE DO MODELO\n",
    "def test_model_with_example(vaga_data, candidato_data):\n",
    "    \"\"\"Testa o modelo com dados de entrada\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== TESTE DO MODELO ===\")\n",
    "    print(f\"Vaga: {vaga_data.get('titulo_vaga', 'N/A')}\")\n",
    "    print(f\"Candidato: {candidato_data.get('nome', 'N/A')}\")\n",
    "    \n",
    "    # Simular processamento como seria feito na aplica√ß√£o\n",
    "    test_row = {**vaga_data, **candidato_data}\n",
    "    \n",
    "    # Aplicar mesma engenharia de features\n",
    "    def calculate_tech_match_test(competencias_req, conhecimentos):\n",
    "        comp_req = str(competencias_req).lower() if competencias_req else ''\n",
    "        conhec = str(conhecimentos).lower() if conhecimentos else ''\n",
    "        \n",
    "        if not comp_req or not conhec:\n",
    "            return 0.0\n",
    "            \n",
    "        comp_words = set(comp_req.split())\n",
    "        conhec_words = set(conhec.split())\n",
    "        \n",
    "        if len(comp_words) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = comp_words.intersection(conhec_words)\n",
    "        return len(intersection) / len(comp_words)\n",
    "    \n",
    "    def calculate_academic_match_test(nivel_vaga, nivel_candidato):\n",
    "        hierarchy = {\n",
    "            'ensino m√©dio': 1, 't√©cnico': 2, 'tecn√≥logo': 3,\n",
    "            'superior': 4, 'p√≥s-gradua√ß√£o': 5, 'mestrado': 6, 'doutorado': 7\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(str(nivel_vaga).lower(), 0)\n",
    "        candidato_level = hierarchy.get(str(nivel_candidato).lower(), 0)\n",
    "        \n",
    "        if vaga_level == 0 or candidato_level == 0:\n",
    "            return 'indefinido'\n",
    "        return 'compat√≠vel' if candidato_level >= vaga_level else 'insuficiente'\n",
    "    \n",
    "    def calculate_english_match_test(nivel_vaga, nivel_candidato):\n",
    "        hierarchy = {\n",
    "            'nenhum': 0, 'b√°sico': 1, 'intermedi√°rio': 2,\n",
    "            'avan√ßado': 3, 'fluente': 4, 'nativo': 5\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(str(nivel_vaga).lower(), -1)\n",
    "        candidato_level = hierarchy.get(str(nivel_candidato).lower(), -1)\n",
    "        \n",
    "        if vaga_level == -1 or candidato_level == -1:\n",
    "            return 'indefinido'\n",
    "        return 'compat√≠vel' if candidato_level >= vaga_level else 'insuficiente'\n",
    "    \n",
    "    # Criar features de teste\n",
    "    test_features = {\n",
    "        'tech_match_score': calculate_tech_match_test(\n",
    "            test_row.get('competencias_tecnicas_requeridas'),\n",
    "            test_row.get('conhecimentos_tecnicos')\n",
    "        ),\n",
    "        'nivel_profissional': str(test_row.get('nivel_profissional', '')).lower(),\n",
    "        'areas_atuacao': str(test_row.get('areas_atuacao', '')).lower(),\n",
    "        'area_de_atuacao': str(test_row.get('area_de_atuacao', '')).lower(),\n",
    "        'academic_match': calculate_academic_match_test(\n",
    "            test_row.get('nivel_academico'),\n",
    "            test_row.get('nivel_academico_candidato', test_row.get('nivel_academico'))\n",
    "        ),\n",
    "        'english_match': calculate_english_match_test(\n",
    "            test_row.get('nivel_ingles'),\n",
    "            test_row.get('nivel_ingles_candidato', test_row.get('nivel_ingles'))\n",
    "        ),\n",
    "        'combined_text': ' '.join([\n",
    "            str(test_row.get('competencias_tecnicas_requeridas', '')),\n",
    "            str(test_row.get('conhecimentos_tecnicos', '')),\n",
    "            str(test_row.get('principais_atividades', '')),\n",
    "            str(test_row.get('area_de_atuacao', ''))\n",
    "        ]).lower().strip()\n",
    "    }\n",
    "    \n",
    "    # Criar DataFrame para predi√ß√£o\n",
    "    test_df = pd.DataFrame([test_features])\n",
    "    \n",
    "    # Fazer predi√ß√£o\n",
    "    prediction = pipeline_enhanced.predict(test_df)[0]\n",
    "    probability = pipeline_enhanced.predict_proba(test_df)[0]\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nFeatures calculadas:\")\n",
    "    for key, value in test_features.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADO DA PREDI√á√ÉO:\")\n",
    "    print(f\"   Predi√ß√£o: {'CONTRATADO' if prediction == 1 else 'N√ÉO CONTRATADO'}\")\n",
    "    print(f\"   Probabilidade de contrata√ß√£o: {probability[1]:.1%}\")\n",
    "    print(f\"   Probabilidade de n√£o contrata√ß√£o: {probability[0]:.1%}\")\n",
    "    print(f\"   Score de compatibilidade: {probability[1]*100:.1f}/100\")\n",
    "    \n",
    "    return {\n",
    "        'prediction': int(prediction),\n",
    "        'probability_hired': float(probability[1]),\n",
    "        'probability_not_hired': float(probability[0]),\n",
    "        'match_score': float(probability[1] * 100),\n",
    "        'features': test_features\n",
    "    }\n",
    "\n",
    "print(\"\\n‚úÖ Modelo salvo e fun√ß√£o de teste criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTE PR√ÅTICO COM EXEMPLO REAL\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTANDO MODELO COM EXEMPLO PR√ÅTICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dados de teste baseados nos dados reais do dataset\n",
    "vaga_exemplo = {\n",
    "    'titulo_vaga': 'Desenvolvedor Python S√™nior',\n",
    "    'competencias_tecnicas_requeridas': 'Python Django Flask PostgreSQL AWS Docker',\n",
    "    'nivel_profissional': 's√™nior',\n",
    "    'areas_atuacao': 'TI - Desenvolvimento',\n",
    "    'principais_atividades': 'desenvolvimento backend apis rest microservices',\n",
    "    'nivel_academico': 'superior',\n",
    "    'nivel_ingles': 'intermedi√°rio'\n",
    "}\n",
    "\n",
    "candidato_exemplo = {\n",
    "    'nome': 'Jo√£o Silva',\n",
    "    'conhecimentos_tecnicos': 'Python Django PostgreSQL Docker Kubernetes Git',\n",
    "    'area_de_atuacao': 'Desenvolvimento Web',\n",
    "    'nivel_academico_candidato': 'superior',\n",
    "    'nivel_ingles_candidato': 'avan√ßado'\n",
    "}\n",
    "\n",
    "# Executar teste\n",
    "try:\n",
    "    resultado = test_model_with_example(vaga_exemplo, candidato_exemplo)\n",
    "    \n",
    "    print(f\"\\nüéØ RESUMO DO TESTE:\")\n",
    "    print(f\"   Score Final: {resultado['match_score']:.1f}%\")\n",
    "    print(f\"   Recomenda√ß√£o: {'ALTA compatibilidade' if resultado['match_score'] >= 70 else 'M√âDIA compatibilidade' if resultado['match_score'] >= 50 else 'BAIXA compatibilidade'}\")\n",
    "    \n",
    "    # Teste com candidato menos compat√≠vel\n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"TESTE COM CANDIDATO MENOS COMPAT√çVEL\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    candidato_menos_compativel = {\n",
    "        'nome': 'Maria Santos',\n",
    "        'conhecimentos_tecnicos': 'Java Spring MySQL',\n",
    "        'area_de_atuacao': 'Desenvolvimento Mobile',\n",
    "        'nivel_academico_candidato': 't√©cnico',\n",
    "        'nivel_ingles_candidato': 'b√°sico'\n",
    "    }\n",
    "    \n",
    "    resultado2 = test_model_with_example(vaga_exemplo, candidato_menos_compativel)\n",
    "    print(f\"\\nüéØ RESUMO DO TESTE 2:\")\n",
    "    print(f\"   Score Final: {resultado2['match_score']:.1f}%\")\n",
    "    print(f\"   Recomenda√ß√£o: {'ALTA compatibilidade' if resultado2['match_score'] >= 70 else 'M√âDIA compatibilidade' if resultado2['match_score'] >= 50 else 'BAIXA compatibilidade'}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ TREINAMENTO CONCLU√çDO COM SUCESSO!\")\n",
    "    print(f\"üìÅ Modelo salvo em: ../app/models/pipeline_aprimorado.joblib\")\n",
    "    print(f\"üìã Metadados salvos em: ../app/models/model_metadata_enhanced.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no teste: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
