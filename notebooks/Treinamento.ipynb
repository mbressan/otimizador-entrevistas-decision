{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf331b43",
   "metadata": {},
   "source": [
    "# Pipeline de Treinamento do Modelo de Otimização de Entrevistas\n",
    "\n",
    "Este notebook implementa o pipeline completo de Machine Learning para o sistema de otimização de entrevistas com modelo aprimorado:\n",
    "\n",
    "1. **Carregamento e Junção de Dados** - Importar vagas, candidatos e prospects da base completa\n",
    "2. **Engenharia de Features Avançadas** - Criar features inteligentes que aproveitam todos os dados disponíveis\n",
    "3. **Treinamento do Modelo Aprimorado** - Treinar um classificador com 7 features avançadas\n",
    "4. **Avaliação e Teste** - Validar a performance e testar com exemplos reais\n",
    "5. **Serialização** - Salvar o modelo treinado para produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822d4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports das bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9996ac6",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Exploração dos Dados\n",
    "\n",
    "Vamos carregar os três arquivos JSON e entender a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9039008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando base completa de dados...\n",
      "Vagas carregadas: 14081\n",
      "Candidatos carregados: 42482\n",
      "Prospects carregados: 14222\n",
      "\n",
      "=== Estrutura das Vagas ===\n",
      "Chaves principais: ['informacoes_basicas', 'perfil_vaga', 'beneficios']\n",
      "Informações básicas: ['data_requicisao', 'limite_esperado_para_contratacao', 'titulo_vaga', 'vaga_sap', 'cliente', 'solicitante_cliente', 'empresa_divisao', 'requisitante', 'analista_responsavel', 'tipo_contratacao', 'prazo_contratacao', 'objetivo_vaga', 'prioridade_vaga', 'origem_vaga', 'superior_imediato', 'nome', 'telefone']\n",
      "Perfil vaga: ['pais', 'estado', 'cidade', 'bairro', 'regiao', 'local_trabalho', 'vaga_especifica_para_pcd', 'faixa_etaria', 'horario_trabalho', 'nivel profissional', 'nivel_academico', 'nivel_ingles', 'nivel_espanhol', 'outro_idioma', 'areas_atuacao', 'principais_atividades', 'competencia_tecnicas_e_comportamentais', 'demais_observacoes', 'viagens_requeridas', 'equipamentos_necessarios']\n"
     ]
    }
   ],
   "source": [
    "# Definir caminhos dos arquivos\n",
    "data_path = Path(\"../data\")\n",
    "\n",
    "# Usar a base completa de dados para treinamento\n",
    "vagas_dev_path = data_path / \"vagas.json\"\n",
    "applicants_dev_path = data_path / \"applicants.json\"\n",
    "prospects_dev_path = data_path / \"prospects.json\"\n",
    "\n",
    "# Função para carregar dados JSON\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Carrega dados de um arquivo JSON\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Carregar base completa de dados\n",
    "print(\"Carregando base completa de dados...\")\n",
    "vagas_data = load_json_data(vagas_dev_path)\n",
    "applicants_data = load_json_data(applicants_dev_path)\n",
    "prospects_data = load_json_data(prospects_dev_path)\n",
    "\n",
    "print(f\"Vagas carregadas: {len(vagas_data)}\")\n",
    "print(f\"Candidatos carregados: {len(applicants_data)}\")\n",
    "print(f\"Prospects carregados: {len(prospects_data)}\")\n",
    "\n",
    "# Explorar estrutura dos dados\n",
    "print(\"\\n=== Estrutura das Vagas ===\")\n",
    "if vagas_data:\n",
    "    vaga_exemplo = list(vagas_data.values())[0]\n",
    "    print(\"Chaves principais:\", list(vaga_exemplo.keys()))\n",
    "    print(\"Informações básicas:\", list(vaga_exemplo.get('informacoes_basicas', {}).keys()))\n",
    "    print(\"Perfil vaga:\", list(vaga_exemplo.get('perfil_vaga', {}).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34be6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados das vagas...\n",
      "Processando dados dos candidatos...\n",
      "\n",
      "DataFrame Vagas: (14081, 13)\n",
      "DataFrame Candidatos: (42482, 12)\n",
      "\n",
      "=== Primeiras vagas ===\n",
      "  id_vaga             titulo_vaga nivel_profissional  \\\n",
      "0    5185        Operation Lead -             Sênior   \n",
      "1    5184  Consultor PP/QM Sênior             Sênior   \n",
      "2    5183   ANALISTA PL/JR C/ SQL           Analista   \n",
      "\n",
      "                       areas_atuacao  \n",
      "0       TI - Sistemas e Ferramentas-  \n",
      "1  TI - Desenvolvimento/Programação-  \n",
      "2       TI - Sistemas e Ferramentas-  \n"
     ]
    }
   ],
   "source": [
    "# Função para normalizar dados das vagas\n",
    "def normalize_vagas(vagas_data):\n",
    "    \"\"\"Normaliza os dados das vagas para um formato estruturado\"\"\"\n",
    "    vagas_list = []\n",
    "    \n",
    "    for vaga_id, vaga_info in vagas_data.items():\n",
    "        try:\n",
    "            basic_info = vaga_info.get('informacoes_basicas', {})\n",
    "            profile_info = vaga_info.get('perfil_vaga', {})\n",
    "            \n",
    "            normalized_vaga = {\n",
    "                'id_vaga': vaga_id,\n",
    "                'titulo_vaga': basic_info.get('titulo_vaga', ''),\n",
    "                'cliente': basic_info.get('cliente', ''),\n",
    "                'tipo_contratacao': basic_info.get('tipo_contratacao', ''),\n",
    "                'nivel_profissional': profile_info.get('nivel profissional', ''),\n",
    "                'nivel_academico': profile_info.get('nivel_academico', ''),\n",
    "                'nivel_ingles': profile_info.get('nivel_ingles', ''),\n",
    "                'areas_atuacao': profile_info.get('areas_atuacao', ''),\n",
    "                'competencias_tecnicas_requeridas': profile_info.get('competencia_tecnicas_e_comportamentais', ''),\n",
    "                'principais_atividades': profile_info.get('principais_atividades', ''),\n",
    "                'pais': profile_info.get('pais', ''),\n",
    "                'estado': profile_info.get('estado', ''),\n",
    "                'cidade': profile_info.get('cidade', '')\n",
    "            }\n",
    "            vagas_list.append(normalized_vaga)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar vaga {vaga_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(vagas_list)\n",
    "\n",
    "# Função para normalizar dados dos candidatos\n",
    "def normalize_applicants(applicants_data):\n",
    "    \"\"\"Normaliza os dados dos candidatos\"\"\"\n",
    "    applicants_list = []\n",
    "    \n",
    "    for candidate_id, candidate_info in applicants_data.items():\n",
    "        try:\n",
    "            basic_info = candidate_info.get('infos_basicas', {})\n",
    "            personal_info = candidate_info.get('informacoes_pessoais', {})\n",
    "            academic_info = candidate_info.get('formacao_academica', {})\n",
    "            professional_info = candidate_info.get('informacoes_profissionais', {})\n",
    "            \n",
    "            normalized_candidate = {\n",
    "                'codigo_candidato': candidate_id,\n",
    "                'nome': basic_info.get('nome', ''),\n",
    "                'email': basic_info.get('email', ''),\n",
    "                'telefone': basic_info.get('telefone', ''),\n",
    "                'data_nascimento': personal_info.get('data_nascimento', ''),\n",
    "                'estado_civil': personal_info.get('estado_civil', ''),\n",
    "                'pcd': personal_info.get('pcd', ''),\n",
    "                'nivel_academico': academic_info.get('nivel_academico', '') if academic_info else '',\n",
    "                'area_formacao': academic_info.get('area_formacao', '') if academic_info else '',\n",
    "                'nivel_ingles': professional_info.get('nivel_ingles', '') if professional_info else '',\n",
    "                'conhecimentos_tecnicos': professional_info.get('conhecimentos_tecnicos', '') if professional_info else '',\n",
    "                'area_de_atuacao': professional_info.get('area_atuacao', '') if professional_info else ''\n",
    "            }\n",
    "            applicants_list.append(normalized_candidate)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar candidato {candidate_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(applicants_list)\n",
    "\n",
    "# Processar os dados\n",
    "print(\"Processando dados das vagas...\")\n",
    "df_vagas = normalize_vagas(vagas_data)\n",
    "\n",
    "print(\"Processando dados dos candidatos...\")\n",
    "df_candidates = normalize_applicants(applicants_data)\n",
    "\n",
    "print(f\"\\nDataFrame Vagas: {df_vagas.shape}\")\n",
    "print(f\"DataFrame Candidatos: {df_candidates.shape}\")\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\n=== Primeiras vagas ===\")\n",
    "print(df_vagas.head(3)[['id_vaga', 'titulo_vaga', 'nivel_profissional', 'areas_atuacao']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7faf4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados dos prospects...\n",
      "DataFrame Prospects: (53759, 8)\n",
      "\n",
      "Distribuição da variável alvo 'contratado':\n",
      "contratado\n",
      "0    51001\n",
      "1     2758\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Taxa de contratação: 5.13%\n",
      "\n",
      "=== Situações dos candidatos ===\n",
      "situacao_candidado\n",
      "Prospect                          20021\n",
      "Encaminhado ao Requisitante       16122\n",
      "Inscrito                           3980\n",
      "Não Aprovado pelo Cliente          3492\n",
      "Contratado pela Decision           2758\n",
      "Desistiu                           2349\n",
      "Não Aprovado pelo RH               1765\n",
      "Não Aprovado pelo Requisitante      765\n",
      "Entrevista Técnica                  579\n",
      "Sem interesse nesta vaga            576\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Função para processar prospects e criar variável alvo\n",
    "def process_prospects(prospects_data):\n",
    "    \"\"\"Processa os dados de prospects para extrair pares candidato-vaga com situação\"\"\"\n",
    "    prospects_list = []\n",
    "    \n",
    "    for vaga_id, vaga_prospect in prospects_data.items():\n",
    "        vaga_titulo = vaga_prospect.get('titulo', '')\n",
    "        prospects = vaga_prospect.get('prospects', [])\n",
    "        \n",
    "        for prospect in prospects:\n",
    "            try:\n",
    "                # Extrair código do candidato (remover prefixos se existirem)\n",
    "                codigo_candidato = prospect.get('codigo', '').strip()\n",
    "                situacao = prospect.get('situacao_candidado', '').strip()\n",
    "                \n",
    "                prospect_record = {\n",
    "                    'id_vaga': vaga_id,\n",
    "                    'codigo_candidato': codigo_candidato,\n",
    "                    'nome_candidato': prospect.get('nome', ''),\n",
    "                    'situacao_candidado': situacao,\n",
    "                    'data_candidatura': prospect.get('data_candidatura', ''),\n",
    "                    'comentario': prospect.get('comentario', ''),\n",
    "                    'recrutador': prospect.get('recrutador', ''),\n",
    "                    # Criar variável alvo binária\n",
    "                    'contratado': 1 if 'Contratado pela Decision' in situacao else 0\n",
    "                }\n",
    "                prospects_list.append(prospect_record)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar prospect da vaga {vaga_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(prospects_list)\n",
    "\n",
    "# Processar prospects\n",
    "print(\"Processando dados dos prospects...\")\n",
    "df_prospects = process_prospects(prospects_data)\n",
    "\n",
    "print(f\"DataFrame Prospects: {df_prospects.shape}\")\n",
    "print(f\"\\nDistribuição da variável alvo 'contratado':\")\n",
    "print(df_prospects['contratado'].value_counts())\n",
    "print(f\"\\nTaxa de contratação: {df_prospects['contratado'].mean():.2%}\")\n",
    "\n",
    "# Visualizar algumas situações de candidatos\n",
    "print(\"\\n=== Situações dos candidatos ===\")\n",
    "print(df_prospects['situacao_candidado'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70043d88",
   "metadata": {},
   "source": [
    "## 2. Combinação e Preparação dos Dados\n",
    "\n",
    "Vamos combinar os três DataFrames para criar um dataset unificado com pares candidato-vaga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6af761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinando os dados...\n",
      "Após join com vagas: (53735, 20)\n",
      "Dataset final: (45071, 31)\n",
      "Taxa de contratação no dataset final: 5.00%\n",
      "\n",
      "=== Colunas disponíveis ===\n",
      " 1. id_vaga\n",
      " 2. codigo_candidato\n",
      " 3. nome_candidato\n",
      " 4. situacao_candidado\n",
      " 5. data_candidatura\n",
      " 6. comentario\n",
      " 7. recrutador\n",
      " 8. contratado\n",
      " 9. titulo_vaga\n",
      "10. cliente\n",
      "11. tipo_contratacao\n",
      "12. nivel_profissional\n",
      "13. nivel_academico_vaga\n",
      "14. nivel_ingles_vaga\n",
      "15. areas_atuacao\n",
      "16. competencias_tecnicas_requeridas\n",
      "17. principais_atividades\n",
      "18. pais\n",
      "19. estado\n",
      "20. cidade\n",
      "21. nome\n",
      "22. email\n",
      "23. telefone\n",
      "24. data_nascimento\n",
      "25. estado_civil\n",
      "26. pcd\n",
      "27. nivel_academico_candidato\n",
      "28. area_formacao\n",
      "29. nivel_ingles_candidato\n",
      "30. conhecimentos_tecnicos\n",
      "31. area_de_atuacao\n",
      "\n",
      "=== Amostra dos dados combinados ===\n",
      "  id_vaga                                        titulo_vaga  \\\n",
      "0    4530                                CONSULTOR CONTROL M   \n",
      "1    4530                                CONSULTOR CONTROL M   \n",
      "2    4531  2021-2607395-PeopleSoft Application Engine-Dom...   \n",
      "3    4531  2021-2607395-PeopleSoft Application Engine-Dom...   \n",
      "4    4533  2021-2605708-Microfocus Application Life Cycle...   \n",
      "\n",
      "             nome_candidato           situacao_candidado  contratado  \n",
      "0               José Vieira  Encaminhado ao Requisitante           0  \n",
      "1  Srta. Isabela Cavalcante  Encaminhado ao Requisitante           0  \n",
      "2     Sra. Yasmin Fernandes     Contratado pela Decision           1  \n",
      "3            Alexia Barbosa  Encaminhado ao Requisitante           0  \n",
      "4            Arthur Almeida     Contratado pela Decision           1  \n"
     ]
    }
   ],
   "source": [
    "# Combinar os DataFrames\n",
    "print(\"Combinando os dados...\")\n",
    "\n",
    "# Primeiro, fazer join prospects com vagas\n",
    "df_combined = df_prospects.merge(\n",
    "    df_vagas, \n",
    "    on='id_vaga', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Após join com vagas: {df_combined.shape}\")\n",
    "\n",
    "# Depois, fazer join com candidatos\n",
    "df_final = df_combined.merge(\n",
    "    df_candidates,\n",
    "    on='codigo_candidato',\n",
    "    how='inner',\n",
    "    suffixes=('_vaga', '_candidato')\n",
    ")\n",
    "\n",
    "print(f\"Dataset final: {df_final.shape}\")\n",
    "print(f\"Taxa de contratação no dataset final: {df_final['contratado'].mean():.2%}\")\n",
    "\n",
    "# Verificar colunas disponíveis\n",
    "print(\"\\n=== Colunas disponíveis ===\")\n",
    "for i, col in enumerate(df_final.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "# Visualizar algumas linhas\n",
    "print(\"\\n=== Amostra dos dados combinados ===\")\n",
    "sample_cols = ['id_vaga', 'titulo_vaga', 'nome_candidato', 'situacao_candidado', 'contratado']\n",
    "print(df_final[sample_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e2e17",
   "metadata": {},
   "source": [
    "## 3. Configuração do Banco de Dados PostgreSQL\n",
    "\n",
    "Vamos criar estruturas de banco de dados PostgreSQL para armazenar os dados de forma eficiente e escalável. O PostgreSQL oferece recursos avançados como índices GIN para busca full-text e melhor performance para aplicações em produção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b587091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÃO DO BANCO DE DADOS POSTGRESQL\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== CONFIGURANDO BANCO DE DADOS POSTGRESQL ===\")\n",
    "\n",
    "# Configurações do PostgreSQL - usando variáveis de ambiente\n",
    "PG_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'otimizador_entrevistas'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432))\n",
    "}\n",
    "\n",
    "print(f\"🔗 Conectando ao PostgreSQL: {PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\")\n",
    "\n",
    "def create_pg_engine():\n",
    "    \"\"\"Cria engine SQLAlchemy para PostgreSQL\"\"\"\n",
    "    connection_string = f\"postgresql://{PG_CONFIG['user']}:{PG_CONFIG['password']}@{PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\"\n",
    "    return create_engine(connection_string)\n",
    "\n",
    "def create_database_schema():\n",
    "    \"\"\"Cria as tabelas do banco PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(\"📋 Criando tabelas PostgreSQL...\")\n",
    "        \n",
    "        # Tabela de Vagas\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS vagas (\n",
    "            id_vaga VARCHAR(50) PRIMARY KEY,\n",
    "            titulo_vaga TEXT,\n",
    "            cliente VARCHAR(200),\n",
    "            tipo_contratacao VARCHAR(100),\n",
    "            nivel_profissional VARCHAR(50),\n",
    "            nivel_academico VARCHAR(50),\n",
    "            nivel_ingles VARCHAR(50),\n",
    "            areas_atuacao TEXT,\n",
    "            competencias_tecnicas_requeridas TEXT,\n",
    "            principais_atividades TEXT,\n",
    "            pais VARCHAR(100),\n",
    "            estado VARCHAR(100),\n",
    "            cidade VARCHAR(100),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de Candidatos\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS candidatos (\n",
    "            codigo_candidato VARCHAR(50) PRIMARY KEY,\n",
    "            nome VARCHAR(200),\n",
    "            email VARCHAR(200),\n",
    "            telefone VARCHAR(50),\n",
    "            data_nascimento VARCHAR(20),\n",
    "            estado_civil VARCHAR(50),\n",
    "            pcd VARCHAR(10),\n",
    "            nivel_academico VARCHAR(50),\n",
    "            area_formacao VARCHAR(200),\n",
    "            nivel_ingles VARCHAR(50),\n",
    "            conhecimentos_tecnicos TEXT,\n",
    "            area_de_atuacao VARCHAR(200),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de Prospects (histórico de candidaturas)\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS prospects (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            id_vaga VARCHAR(50),\n",
    "            codigo_candidato VARCHAR(50),\n",
    "            nome_candidato VARCHAR(200),\n",
    "            situacao_candidado VARCHAR(200),\n",
    "            data_candidatura VARCHAR(20),\n",
    "            comentario TEXT,\n",
    "            recrutador VARCHAR(200),\n",
    "            contratado INTEGER,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (id_vaga) REFERENCES vagas(id_vaga) ON DELETE CASCADE,\n",
    "            FOREIGN KEY (codigo_candidato) REFERENCES candidatos(codigo_candidato) ON DELETE CASCADE\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Tabela de Predições (log das predições do ML)\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS predicoes (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            id_vaga VARCHAR(50),\n",
    "            codigo_candidato VARCHAR(50),\n",
    "            tech_match_score REAL,\n",
    "            academic_match VARCHAR(50),\n",
    "            english_match VARCHAR(50),\n",
    "            probabilidade_contratacao REAL,\n",
    "            predicao_contratado INTEGER,\n",
    "            modelo_versao VARCHAR(50),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (id_vaga) REFERENCES vagas(id_vaga) ON DELETE CASCADE,\n",
    "            FOREIGN KEY (codigo_candidato) REFERENCES candidatos(codigo_candidato) ON DELETE CASCADE\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Índices otimizados para PostgreSQL\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vagas_nivel ON vagas(nivel_profissional)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vagas_areas ON vagas USING gin(to_tsvector(\\'portuguese\\', areas_atuacao))')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_candidatos_area ON candidatos USING gin(to_tsvector(\\'portuguese\\', area_de_atuacao))')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_prospects_vaga ON prospects(id_vaga)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_prospects_candidato ON prospects(codigo_candidato)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_prospects_contratado ON prospects(contratado)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_predicoes_vaga ON predicoes(id_vaga)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_predicoes_data ON predicoes(created_at)')\n",
    "        \n",
    "        # Views úteis para PostgreSQL\n",
    "        cursor.execute('''\n",
    "        CREATE OR REPLACE VIEW vw_vagas_stats AS\n",
    "        SELECT \n",
    "            v.id_vaga,\n",
    "            v.titulo_vaga,\n",
    "            v.cliente,\n",
    "            v.nivel_profissional,\n",
    "            v.areas_atuacao,\n",
    "            COUNT(p.codigo_candidato) as total_candidatos,\n",
    "            SUM(p.contratado) as total_contratados,\n",
    "            CASE \n",
    "                WHEN COUNT(p.codigo_candidato) > 0 \n",
    "                THEN ROUND(AVG(p.contratado) * 100, 2) \n",
    "                ELSE 0 \n",
    "            END as taxa_contratacao\n",
    "        FROM vagas v\n",
    "        LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "        GROUP BY v.id_vaga, v.titulo_vaga, v.cliente, v.nivel_profissional, v.areas_atuacao\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "        CREATE OR REPLACE VIEW vw_candidatos_ranking AS\n",
    "        SELECT \n",
    "            c.codigo_candidato,\n",
    "            c.nome,\n",
    "            c.area_de_atuacao,\n",
    "            c.nivel_academico,\n",
    "            COUNT(p.id_vaga) as total_candidaturas,\n",
    "            SUM(p.contratado) as total_contratacoes,\n",
    "            CASE \n",
    "                WHEN COUNT(p.id_vaga) > 0 \n",
    "                THEN ROUND(AVG(p.contratado) * 100, 2) \n",
    "                ELSE 0 \n",
    "            END as taxa_sucesso\n",
    "        FROM candidatos c\n",
    "        LEFT JOIN prospects p ON c.codigo_candidato = p.codigo_candidato\n",
    "        GROUP BY c.codigo_candidato, c.nome, c.area_de_atuacao, c.nivel_academico\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"✅ Esquema PostgreSQL criado com sucesso!\")\n",
    "        \n",
    "        # Verificar tabelas criadas\n",
    "        cursor.execute(\"\"\"\n",
    "        SELECT tablename \n",
    "        FROM pg_tables \n",
    "        WHERE schemaname = 'public' \n",
    "        AND tablename IN ('vagas', 'candidatos', 'prospects', 'predicoes')\n",
    "        \"\"\")\n",
    "        tabelas = cursor.fetchall()\n",
    "        print(f\"📊 Tabelas criadas: {[t[0] for t in tabelas]}\")\n",
    "        \n",
    "        # Verificar views criadas\n",
    "        cursor.execute(\"\"\"\n",
    "        SELECT viewname \n",
    "        FROM pg_views \n",
    "        WHERE schemaname = 'public' \n",
    "        AND viewname LIKE 'vw_%'\n",
    "        \"\"\")\n",
    "        views = cursor.fetchall()\n",
    "        print(f\"👁️  Views criadas: {[v[0] for v in views]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao criar esquema PostgreSQL: {e}\")\n",
    "        print(\"💡 Certifique-se de que:\")\n",
    "        print(\"   - PostgreSQL está rodando\")\n",
    "        print(\"   - Banco 'otimizador_entrevistas' existe\")\n",
    "        print(\"   - Credenciais estão corretas\")\n",
    "        print(\"   - psycopg2-binary está instalado: pip install psycopg2-binary\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Testar conexão e criar esquema\n",
    "try:\n",
    "    create_database_schema()\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  Para configurar PostgreSQL:\")\n",
    "    print(\"1. Instale PostgreSQL: https://www.postgresql.org/download/\")\n",
    "    print(\"2. Crie o banco: createdb otimizador_entrevistas\")\n",
    "    print(\"3. Configure variáveis de ambiente:\")\n",
    "    print(\"   export DB_HOST=localhost\")\n",
    "    print(\"   export DB_NAME=otimizador_entrevistas\") \n",
    "    print(\"   export DB_USER=postgres\")\n",
    "    print(\"   export DB_PASSWORD=sua_senha\")\n",
    "    print(\"4. Instale driver: pip install psycopg2-binary sqlalchemy\")\n",
    "    print(f\"\\nErro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGRAÇÃO DOS DADOS JSON PARA POSTGRESQL\n",
    "def migrate_data_to_postgresql():\n",
    "    \"\"\"Migra os dados dos DataFrames para o banco PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        \n",
    "        print(\"🚀 Migrando dados para PostgreSQL...\")\n",
    "        \n",
    "        # Verificar se já existem dados\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(\"SELECT COUNT(*) FROM vagas\").fetchone()\n",
    "            vagas_count = result[0]\n",
    "        \n",
    "        if vagas_count > 0:\n",
    "            print(f\"⚠️  Banco já contém {vagas_count} vagas. Pulando migração.\")\n",
    "            print(\"   Para recriar os dados, execute: TRUNCATE TABLE prospects, candidatos, vagas RESTART IDENTITY CASCADE;\")\n",
    "            return\n",
    "        \n",
    "        # 1. Migrar Vagas\n",
    "        print(\"📋 Migrando vagas para PostgreSQL...\")\n",
    "        df_vagas_clean = df_vagas.fillna('')  # Tratar valores nulos\n",
    "        \n",
    "        # Usar pandas to_sql com PostgreSQL\n",
    "        rows_inserted = df_vagas_clean.to_sql(\n",
    "            'vagas', \n",
    "            engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            method='multi',  # Mais eficiente para grandes volumes\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"✅ {len(df_vagas)} vagas migradas\")\n",
    "        \n",
    "        # 2. Migrar Candidatos\n",
    "        print(\"👥 Migrando candidatos para PostgreSQL...\")\n",
    "        df_candidates_clean = df_candidates.fillna('')  # Tratar valores nulos\n",
    "        \n",
    "        rows_inserted = df_candidates_clean.to_sql(\n",
    "            'candidatos', \n",
    "            engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"✅ {len(df_candidates)} candidatos migrados\")\n",
    "        \n",
    "        # 3. Migrar Prospects\n",
    "        print(\"🎯 Migrando prospects para PostgreSQL...\")\n",
    "        df_prospects_clean = df_prospects.fillna('')  # Tratar valores nulos\n",
    "        \n",
    "        rows_inserted = df_prospects_clean.to_sql(\n",
    "            'prospects', \n",
    "            engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"✅ {len(df_prospects)} prospects migrados\")\n",
    "        \n",
    "        # Verificar migração usando pandas\n",
    "        with engine.connect() as conn:\n",
    "            vagas_df = pd.read_sql(\"SELECT COUNT(*) as count FROM vagas\", conn)\n",
    "            candidatos_df = pd.read_sql(\"SELECT COUNT(*) as count FROM candidatos\", conn)\n",
    "            prospects_df = pd.read_sql(\"SELECT COUNT(*) as count FROM prospects\", conn)\n",
    "            contratados_df = pd.read_sql(\"SELECT COUNT(*) as count FROM prospects WHERE contratado = 1\", conn)\n",
    "            \n",
    "            vagas_db = vagas_df['count'].iloc[0]\n",
    "            candidatos_db = candidatos_df['count'].iloc[0]\n",
    "            prospects_db = prospects_df['count'].iloc[0]\n",
    "            contratados_db = contratados_df['count'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n📊 RESUMO DA MIGRAÇÃO POSTGRESQL:\")\n",
    "        print(f\"   Vagas no banco: {vagas_db:,}\")\n",
    "        print(f\"   Candidatos no banco: {candidatos_db:,}\")\n",
    "        print(f\"   Prospects no banco: {prospects_db:,}\")\n",
    "        print(f\"   Contratados: {contratados_db:,}\")\n",
    "        print(f\"   Taxa de contratação: {contratados_db/prospects_db:.2%}\")\n",
    "        \n",
    "        # Informações do banco PostgreSQL\n",
    "        with engine.connect() as conn:\n",
    "            # Tamanho das tabelas\n",
    "            size_query = \"\"\"\n",
    "            SELECT \n",
    "                schemaname,\n",
    "                tablename,\n",
    "                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
    "            FROM pg_tables \n",
    "            WHERE schemaname = 'public' \n",
    "            AND tablename IN ('vagas', 'candidatos', 'prospects', 'predicoes')\n",
    "            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n",
    "            \"\"\"\n",
    "            sizes_df = pd.read_sql(size_query, conn)\n",
    "            \n",
    "            print(f\"\\n\udcbe TAMANHO DAS TABELAS:\")\n",
    "            for _, row in sizes_df.iterrows():\n",
    "                print(f\"   {row['tablename']}: {row['size']}\")\n",
    "        \n",
    "        print(f\"\\n🎉 Migração PostgreSQL concluída com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na migração PostgreSQL: {e}\")\n",
    "        print(f\"💡 Verifique se o PostgreSQL está rodando e as credenciais estão corretas\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Executar migração PostgreSQL\n",
    "migrate_data_to_postgresql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES UTILITÁRIAS PARA CONSULTAS POSTGRESQL\n",
    "def query_postgresql_stats():\n",
    "    \"\"\"Consulta estatísticas do banco PostgreSQL usando pandas\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        print(\"=== ESTATÍSTICAS DO BANCO POSTGRESQL ===\")\n",
    "        \n",
    "        # Estatísticas básicas usando pandas\n",
    "        with engine.connect() as conn:\n",
    "            # Contadores básicos\n",
    "            stats_query = \"\"\"\n",
    "            SELECT \n",
    "                'Total de Vagas' as metrica,\n",
    "                (SELECT COUNT(*) FROM vagas) as valor\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Total de Candidatos',\n",
    "                (SELECT COUNT(*) FROM candidatos)\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Total de Prospects',\n",
    "                (SELECT COUNT(*) FROM prospects)\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Candidatos Contratados',\n",
    "                (SELECT COUNT(*) FROM prospects WHERE contratado = 1)\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'Vagas com Prospects',\n",
    "                (SELECT COUNT(DISTINCT id_vaga) FROM prospects)\n",
    "            \"\"\"\n",
    "            \n",
    "            stats_df = pd.read_sql(stats_query, conn)\n",
    "            \n",
    "            for _, row in stats_df.iterrows():\n",
    "                print(f\"📊 {row['metrica']}: {row['valor']:,}\")\n",
    "        \n",
    "        # Top 5 vagas com mais candidatos\n",
    "        print(f\"\\n=== TOP 5 VAGAS COM MAIS CANDIDATOS ===\")\n",
    "        with engine.connect() as conn:\n",
    "            top_vagas_query = '''\n",
    "            SELECT \n",
    "                v.titulo_vaga,\n",
    "                v.cliente,\n",
    "                COUNT(p.codigo_candidato) as total_candidatos\n",
    "            FROM vagas v\n",
    "            LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "            GROUP BY v.id_vaga, v.titulo_vaga, v.cliente\n",
    "            ORDER BY total_candidatos DESC\n",
    "            LIMIT 5\n",
    "            '''\n",
    "            \n",
    "            top_vagas_df = pd.read_sql(top_vagas_query, conn)\n",
    "            \n",
    "            for _, row in top_vagas_df.iterrows():\n",
    "                titulo = row['titulo_vaga'][:40]\n",
    "                cliente = row['cliente'][:20]\n",
    "                total = row['total_candidatos']\n",
    "                print(f\"   {titulo:<40} | {cliente:<20} | {total:,} candidatos\")\n",
    "        \n",
    "        # Distribuição por nível profissional\n",
    "        print(f\"\\n=== DISTRIBUIÇÃO POR NÍVEL PROFISSIONAL ===\")\n",
    "        with engine.connect() as conn:\n",
    "            nivel_query = '''\n",
    "            SELECT \n",
    "                nivel_profissional, \n",
    "                COUNT(*) as total\n",
    "            FROM vagas\n",
    "            WHERE nivel_profissional != '' AND nivel_profissional IS NOT NULL\n",
    "            GROUP BY nivel_profissional\n",
    "            ORDER BY total DESC\n",
    "            '''\n",
    "            \n",
    "            nivel_df = pd.read_sql(nivel_query, conn)\n",
    "            \n",
    "            for _, row in nivel_df.iterrows():\n",
    "                print(f\"   {row['nivel_profissional']:<20} | {row['total']:,} vagas\")\n",
    "        \n",
    "        # Taxa de contratação por área (usando HAVING otimizado para PostgreSQL)\n",
    "        print(f\"\\n=== TAXA DE CONTRATAÇÃO POR ÁREA ===\")\n",
    "        with engine.connect() as conn:\n",
    "            area_query = '''\n",
    "            SELECT \n",
    "                v.areas_atuacao,\n",
    "                COUNT(p.codigo_candidato) as total_candidatos,\n",
    "                SUM(p.contratado) as contratados,\n",
    "                ROUND(AVG(p.contratado::numeric) * 100, 2) as taxa_contratacao\n",
    "            FROM vagas v\n",
    "            LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "            WHERE v.areas_atuacao != '' AND v.areas_atuacao IS NOT NULL\n",
    "            GROUP BY v.areas_atuacao\n",
    "            HAVING COUNT(p.codigo_candidato) >= 50\n",
    "            ORDER BY taxa_contratacao DESC\n",
    "            LIMIT 10\n",
    "            '''\n",
    "            \n",
    "            area_df = pd.read_sql(area_query, conn)\n",
    "            \n",
    "            for _, row in area_df.iterrows():\n",
    "                area = row['areas_atuacao'][:30]\n",
    "                total_cand = row['total_candidatos']\n",
    "                contratados = row['contratados']\n",
    "                taxa = row['taxa_contratacao']\n",
    "                print(f\"   {area:<30} | {total_cand:,} candidatos | {contratados} contratados | {taxa}%\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao consultar estatísticas PostgreSQL: {e}\")\n",
    "\n",
    "def validate_postgresql_consistency():\n",
    "    \"\"\"Valida a consistência dos dados no PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        print(\"\\n=== VALIDAÇÃO DE CONSISTÊNCIA POSTGRESQL ===\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            # Verificar chaves estrangeiras órfãs\n",
    "            orphan_query = '''\n",
    "            SELECT COUNT(*) as orphans FROM prospects p\n",
    "            WHERE p.id_vaga NOT IN (SELECT id_vaga FROM vagas)\n",
    "            OR p.codigo_candidato NOT IN (SELECT codigo_candidato FROM candidatos)\n",
    "            '''\n",
    "            \n",
    "            orphan_df = pd.read_sql(orphan_query, conn)\n",
    "            orphans = orphan_df['orphans'].iloc[0]\n",
    "            \n",
    "            if orphans == 0:\n",
    "                print(\"✅ Todas as chaves estrangeiras estão consistentes\")\n",
    "            else:\n",
    "                print(f\"⚠️  {orphans} prospects com chaves estrangeiras inválidas\")\n",
    "            \n",
    "            # Verificar duplicatas usando PostgreSQL específico\n",
    "            dup_query = '''\n",
    "            SELECT \n",
    "                'vagas' as tabela,\n",
    "                COUNT(*) - COUNT(DISTINCT id_vaga) as duplicatas\n",
    "            FROM vagas\n",
    "            UNION ALL\n",
    "            SELECT \n",
    "                'candidatos',\n",
    "                COUNT(*) - COUNT(DISTINCT codigo_candidato)\n",
    "            FROM candidatos\n",
    "            '''\n",
    "            \n",
    "            dup_df = pd.read_sql(dup_query, conn)\n",
    "            total_dups = dup_df['duplicatas'].sum()\n",
    "            \n",
    "            if total_dups == 0:\n",
    "                print(\"✅ Não há duplicatas nas chaves primárias\")\n",
    "            else:\n",
    "                print(f\"⚠️  {total_dups} duplicatas encontradas\")\n",
    "                for _, row in dup_df.iterrows():\n",
    "                    if row['duplicatas'] > 0:\n",
    "                        print(f\"    {row['tabela']}: {row['duplicatas']} duplicatas\")\n",
    "            \n",
    "            # Verificar valores nulos em campos críticos\n",
    "            null_checks = {\n",
    "                'vagas.titulo_vaga': \"SELECT COUNT(*) FROM vagas WHERE titulo_vaga IS NULL OR titulo_vaga = ''\",\n",
    "                'candidatos.nome': \"SELECT COUNT(*) FROM candidatos WHERE nome IS NULL OR nome = ''\",\n",
    "                'prospects.situacao_candidado': \"SELECT COUNT(*) FROM prospects WHERE situacao_candidado IS NULL OR situacao_candidado = ''\"\n",
    "            }\n",
    "            \n",
    "            all_good = True\n",
    "            for campo, query in null_checks.items():\n",
    "                null_df = pd.read_sql(query, conn)\n",
    "                nulls = null_df.iloc[0, 0]\n",
    "                if nulls == 0:\n",
    "                    print(f\"✅ {campo}: Sem valores nulos/vazios\")\n",
    "                else:\n",
    "                    print(f\"⚠️  {campo}: {nulls} registros com valores nulos/vazios\")\n",
    "                    all_good = False\n",
    "            \n",
    "            # Verificar índices PostgreSQL\n",
    "            indexes_query = '''\n",
    "            SELECT \n",
    "                indexname,\n",
    "                tablename,\n",
    "                indexdef\n",
    "            FROM pg_indexes \n",
    "            WHERE schemaname = 'public' \n",
    "            AND tablename IN ('vagas', 'candidatos', 'prospects', 'predicoes')\n",
    "            ORDER BY tablename, indexname\n",
    "            '''\n",
    "            \n",
    "            indexes_df = pd.read_sql(indexes_query, conn)\n",
    "            print(f\"\\n📋 ÍNDICES POSTGRESQL: {len(indexes_df)} índices criados\")\n",
    "            \n",
    "            # Mostrar índices GIN (específicos do PostgreSQL)\n",
    "            gin_indexes = indexes_df[indexes_df['indexdef'].str.contains('gin', case=False)]\n",
    "            if len(gin_indexes) > 0:\n",
    "                print(\"🔍 Índices GIN para busca full-text:\")\n",
    "                for _, idx in gin_indexes.iterrows():\n",
    "                    print(f\"   {idx['tablename']}.{idx['indexname']}\")\n",
    "            \n",
    "            if all_good:\n",
    "                print(\"\\n🎉 Banco PostgreSQL está consistente e pronto para uso!\")\n",
    "            else:\n",
    "                print(\"\\n⚠️  Algumas inconsistências encontradas - revisar dados se necessário\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na validação PostgreSQL: {e}\")\n",
    "\n",
    "def test_postgresql_advanced_features():\n",
    "    \"\"\"Testa recursos avançados do PostgreSQL\"\"\"\n",
    "    \n",
    "    try:\n",
    "        engine = create_pg_engine()\n",
    "        print(\"\\n=== TESTANDO RECURSOS AVANÇADOS POSTGRESQL ===\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            # Teste de busca full-text com GIN\n",
    "            fulltext_query = '''\n",
    "            SELECT \n",
    "                titulo_vaga,\n",
    "                areas_atuacao,\n",
    "                ts_rank(to_tsvector('portuguese', areas_atuacao), \n",
    "                       to_tsquery('portuguese', 'desenvolvimento')) as rank\n",
    "            FROM vagas \n",
    "            WHERE to_tsvector('portuguese', areas_atuacao) @@ to_tsquery('portuguese', 'desenvolvimento')\n",
    "            ORDER BY rank DESC\n",
    "            LIMIT 5\n",
    "            '''\n",
    "            \n",
    "            try:\n",
    "                fulltext_df = pd.read_sql(fulltext_query, conn)\n",
    "                print(f\"✅ Busca full-text funcionando: {len(fulltext_df)} resultados para 'desenvolvimento'\")\n",
    "                if len(fulltext_df) > 0:\n",
    "                    print(\"   Top resultado:\", fulltext_df.iloc[0]['titulo_vaga'][:50])\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Busca full-text: {e}\")\n",
    "            \n",
    "            # Teste das views criadas\n",
    "            view_test_query = \"SELECT COUNT(*) as total FROM vw_vagas_stats WHERE total_candidatos > 0\"\n",
    "            try:\n",
    "                view_df = pd.read_sql(view_test_query, conn)\n",
    "                print(f\"✅ View vw_vagas_stats funcionando: {view_df['total'].iloc[0]} vagas com candidatos\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  View test: {e}\")\n",
    "            \n",
    "            # Informações do banco PostgreSQL\n",
    "            db_info_query = '''\n",
    "            SELECT \n",
    "                current_database() as database_name,\n",
    "                version() as postgresql_version,\n",
    "                current_user as current_user,\n",
    "                inet_server_addr() as server_ip,\n",
    "                inet_server_port() as server_port\n",
    "            '''\n",
    "            \n",
    "            db_info_df = pd.read_sql(db_info_query, conn)\n",
    "            print(f\"\\n🗄️  INFORMAÇÕES DO BANCO:\")\n",
    "            print(f\"   Database: {db_info_df['database_name'].iloc[0]}\")\n",
    "            print(f\"   PostgreSQL: {db_info_df['postgresql_version'].iloc[0].split(' on ')[0]}\")\n",
    "            print(f\"   Usuário: {db_info_df['current_user'].iloc[0]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro nos testes avançados: {e}\")\n",
    "\n",
    "# Executar todas as consultas e validações PostgreSQL\n",
    "query_postgresql_stats()\n",
    "validate_postgresql_consistency()\n",
    "test_postgresql_advanced_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53310f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÃO DA APLICAÇÃO FLASK PARA POSTGRESQL\n",
    "def generate_flask_postgresql_config():\n",
    "    \"\"\"Gera configuração para a aplicação Flask usar PostgreSQL\"\"\"\n",
    "    \n",
    "    # Código de configuração para app.py\n",
    "    flask_config_code = '''\n",
    "# Configuração PostgreSQL para app.py\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "# Configurações do PostgreSQL\n",
    "PG_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST', 'localhost'),\n",
    "    'database': os.getenv('DB_NAME', 'otimizador_entrevistas'),\n",
    "    'user': os.getenv('DB_USER', 'postgres'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'postgres'),\n",
    "    'port': int(os.getenv('DB_PORT', 5432))\n",
    "}\n",
    "\n",
    "def create_pg_engine():\n",
    "    \"\"\"Cria engine SQLAlchemy para PostgreSQL\"\"\"\n",
    "    connection_string = f\"postgresql://{PG_CONFIG['user']}:{PG_CONFIG['password']}@{PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\"\n",
    "    return create_engine(connection_string, pool_pre_ping=True, pool_recycle=300)\n",
    "\n",
    "def load_vagas_from_postgres():\n",
    "    \"\"\"Carrega vagas do PostgreSQL com filtro de prospects\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT v.* \n",
    "    FROM vagas v \n",
    "    INNER JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "    ORDER BY v.titulo_vaga\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "def load_candidates_from_postgres():\n",
    "    \"\"\"Carrega candidatos do PostgreSQL\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    return pd.read_sql(\"SELECT * FROM candidatos ORDER BY nome\", engine)\n",
    "\n",
    "def load_prospects_from_postgres():\n",
    "    \"\"\"Carrega prospects do PostgreSQL\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    return pd.read_sql(\"SELECT * FROM prospects ORDER BY created_at DESC\", engine)\n",
    "\n",
    "def get_vagas_com_prospects():\n",
    "    \"\"\"Retorna set de vagas que têm prospects\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    query = \"SELECT DISTINCT id_vaga FROM prospects\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return set(df['id_vaga'].tolist())\n",
    "\n",
    "def save_predicao_to_postgres(predicao_data):\n",
    "    \"\"\"Salva predição no PostgreSQL\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    df = pd.DataFrame([predicao_data])\n",
    "    df.to_sql('predicoes', engine, if_exists='append', index=False)\n",
    "    print(f\"✅ Predição salva no PostgreSQL para vaga {predicao_data.get('id_vaga')}\")\n",
    "\n",
    "def get_vaga_candidatos_postgres(id_vaga, limit=50):\n",
    "    \"\"\"Busca candidatos para uma vaga específica com paginação\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    query = \"\"\"\n",
    "    SELECT c.*, p.situacao_candidado, p.contratado\n",
    "    FROM candidatos c\n",
    "    INNER JOIN prospects p ON c.codigo_candidato = p.codigo_candidato\n",
    "    WHERE p.id_vaga = %(id_vaga)s\n",
    "    ORDER BY p.contratado DESC, c.nome\n",
    "    LIMIT %(limit)s\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine, params={'id_vaga': id_vaga, 'limit': limit})\n",
    "\n",
    "# Função para busca otimizada com PostgreSQL\n",
    "def search_vagas_postgres(search_term=None, nivel_filter=None, page=1, per_page=10):\n",
    "    \"\"\"Busca vagas com filtros e paginação otimizada\"\"\"\n",
    "    engine = create_pg_engine()\n",
    "    \n",
    "    base_query = \"\"\"\n",
    "    SELECT v.*, COUNT(p.codigo_candidato) as total_candidatos\n",
    "    FROM vagas v\n",
    "    LEFT JOIN prospects p ON v.id_vaga = p.id_vaga\n",
    "    WHERE 1=1\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    if search_term:\n",
    "        base_query += \" AND (v.titulo_vaga ILIKE %(search)s OR v.cliente ILIKE %(search)s)\"\n",
    "        params['search'] = f'%{search_term}%'\n",
    "    \n",
    "    if nivel_filter:\n",
    "        base_query += \" AND v.nivel_profissional = %(nivel)s\"\n",
    "        params['nivel'] = nivel_filter\n",
    "    \n",
    "    base_query += \" GROUP BY v.id_vaga\"\n",
    "    base_query += \" HAVING COUNT(p.codigo_candidato) > 0\"\n",
    "    base_query += \" ORDER BY total_candidatos DESC\"\n",
    "    \n",
    "    # Paginação\n",
    "    offset = (page - 1) * per_page\n",
    "    paginated_query = base_query + f\" LIMIT {per_page} OFFSET {offset}\"\n",
    "    \n",
    "    return pd.read_sql(paginated_query, engine, params=params)\n",
    "'''\n",
    "    \n",
    "    # Salvar configuração\n",
    "    config_file_path = Path(\"../app/postgresql_integration.py\")\n",
    "    with open(config_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(flask_config_code)\n",
    "    \n",
    "    print(f\"\udc0d Configuração Flask-PostgreSQL gerada: {config_file_path}\")\n",
    "    \n",
    "    # Arquivo .env de exemplo\n",
    "    env_example = '''# Configurações PostgreSQL para .env\n",
    "DB_HOST=localhost\n",
    "DB_NAME=otimizador_entrevistas\n",
    "DB_USER=postgres\n",
    "DB_PASSWORD=sua_senha_aqui\n",
    "DB_PORT=5432\n",
    "\n",
    "# Para produção, use valores seguros:\n",
    "# DB_HOST=seu_servidor_postgres\n",
    "# DB_PASSWORD=senha_forte_aleatoria\n",
    "'''\n",
    "    \n",
    "    env_file_path = Path(\"../app/.env.example\")\n",
    "    with open(env_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(env_example)\n",
    "    \n",
    "    print(f\"📄 Arquivo .env.example criado: {env_file_path}\")\n",
    "    \n",
    "    # Docker Compose para PostgreSQL\n",
    "    docker_compose = '''version: '3.8'\n",
    "\n",
    "services:\n",
    "  postgresql:\n",
    "    image: postgres:15-alpine\n",
    "    container_name: otimizador_postgres\n",
    "    environment:\n",
    "      POSTGRES_DB: otimizador_entrevistas\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: postgres\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n",
    "    restart: unless-stopped\n",
    "\n",
    "  app:\n",
    "    build: .\n",
    "    container_name: otimizador_app\n",
    "    depends_on:\n",
    "      - postgresql\n",
    "    environment:\n",
    "      DB_HOST: postgresql\n",
    "      DB_NAME: otimizador_entrevistas\n",
    "      DB_USER: postgres\n",
    "      DB_PASSWORD: postgres\n",
    "      DB_PORT: 5432\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "'''\n",
    "    \n",
    "    docker_file_path = Path(\"../docker-compose.postgres.yml\")\n",
    "    with open(docker_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(docker_compose)\n",
    "    \n",
    "    print(f\"\udc33 Docker Compose PostgreSQL criado: {docker_file_path}\")\n",
    "    \n",
    "    print(f\"\\n📋 PRÓXIMOS PASSOS PARA INTEGRAÇÃO:\")\n",
    "    print(\"1. Instale dependências: pip install psycopg2-binary sqlalchemy\")\n",
    "    print(\"2. Configure .env: cp app/.env.example app/.env\")\n",
    "    print(\"3. Inicie PostgreSQL: docker-compose -f docker-compose.postgres.yml up -d postgresql\")\n",
    "    print(\"4. Execute este notebook para migrar os dados\")\n",
    "    print(\"5. Integre postgresql_integration.py no app.py\")\n",
    "    print(\"6. Teste a aplicação: python app/app.py\")\n",
    "\n",
    "# Gerar configurações Flask-PostgreSQL\n",
    "print(\"\\n=== GERANDO CONFIGURAÇÕES FLASK-POSTGRESQL ===\")\n",
    "generate_flask_postgresql_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5489f",
   "metadata": {},
   "source": [
    "## 4. Engenharia de Features\n",
    "\n",
    "Vamos criar features relevantes para o modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d710a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando features...\n",
      "Features categóricas disponíveis: ['nivel_profissional', 'nivel_academico_vaga', 'nivel_ingles_vaga', 'areas_atuacao', 'tipo_contratacao', 'nivel_academico_candidato', 'nivel_ingles_candidato', 'area_de_atuacao']\n",
      "Tamanho do dataset: (45071, 32)\n",
      "\n",
      "=== Distribuição de features categóricas ===\n",
      "\n",
      "nivel_profissional:\n",
      "nivel_profissional\n",
      "sênior          17987\n",
      "analista        14684\n",
      "pleno            8597\n",
      "júnior           1465\n",
      "especialista      912\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nivel_academico_vaga:\n",
      "nivel_academico_vaga\n",
      "ensino superior completo      33830\n",
      "ensino médio completo          5391\n",
      "ensino técnico completo        3501\n",
      "ensino superior cursando       1441\n",
      "ensino superior incompleto      520\n",
      "Name: count, dtype: int64\n",
      "\n",
      "nivel_ingles_vaga:\n",
      "nivel_ingles_vaga\n",
      "básico           14913\n",
      "nenhum           11230\n",
      "avançado          8736\n",
      "fluente           4933\n",
      "intermediário     4561\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preparar features básicas para o modelo\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepara as features básicas para o modelo\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Tratar valores nulos e padronizar\n",
    "    df_features = df_features.fillna('')\n",
    "    \n",
    "    # Criar feature de texto combinada (competências + conhecimentos)\n",
    "    df_features['competencias_combinadas'] = (\n",
    "        df_features['competencias_tecnicas_requeridas'].astype(str) + ' ' +\n",
    "        df_features['conhecimentos_tecnicos'].astype(str) + ' ' +\n",
    "        df_features['principais_atividades'].astype(str)\n",
    "    ).str.lower().str.strip()\n",
    "    \n",
    "    # Verificar colunas disponíveis após merge\n",
    "    print(f\"Colunas disponíveis no DataFrame: {list(df_features.columns)}\")\n",
    "    \n",
    "    # Definir features categóricas baseadas nas colunas que realmente existem\n",
    "    potential_categorical = [\n",
    "        'nivel_profissional', \n",
    "        'nivel_academico',  # Pode ser da vaga ou candidato\n",
    "        'nivel_ingles',     # Pode ser da vaga ou candidato  \n",
    "        'areas_atuacao', \n",
    "        'tipo_contratacao', \n",
    "        'area_de_atuacao'\n",
    "    ]\n",
    "    \n",
    "    # Verificar quais colunas realmente existem\n",
    "    available_categorical = []\n",
    "    for feature in potential_categorical:\n",
    "        if feature in df_features.columns:\n",
    "            available_categorical.append(feature)\n",
    "            # Limpar e padronizar\n",
    "            df_features[feature] = df_features[feature].astype(str).str.strip().str.lower()\n",
    "            df_features[feature] = df_features[feature].replace('', 'não_informado')\n",
    "    \n",
    "    print(f\"Features categóricas identificadas: {available_categorical}\")\n",
    "    \n",
    "    return df_features, available_categorical\n",
    "\n",
    "# Preparar features básicas\n",
    "print(\"Preparando features básicas...\")\n",
    "df_features, categorical_cols = prepare_features(df_final)\n",
    "\n",
    "print(f\"Features categóricas disponíveis: {categorical_cols}\")\n",
    "print(f\"Tamanho do dataset: {df_features.shape}\")\n",
    "\n",
    "# Verificar distribuição de algumas features categóricas\n",
    "print(\"\\n=== Distribuição de features categóricas ===\")\n",
    "for col in categorical_cols[:3]:  # Mostrar só as primeiras 3\n",
    "    if col in df_features.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df_features[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404462b",
   "metadata": {},
   "source": [
    "### 4.2. Features Avançadas\n",
    "\n",
    "Agora vamos criar features inteligentes que capturam a compatibilidade entre vagas e candidatos:\n",
    "\n",
    "1. **Tech Match Score**: Compatibilidade técnica baseada em sobreposição de palavras-chave\n",
    "2. **Academic Match**: Compatibilidade de nível acadêmico usando hierarquia\n",
    "3. **English Match**: Compatibilidade de nível de inglês\n",
    "4. **Combined Text**: Texto unificado para análise semântica com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIAÇÃO DE FEATURES AVANÇADAS\n",
    "print(\"=== CRIANDO FEATURES AVANÇADAS ===\")\n",
    "print(\"Baseado na análise dos dados, vamos criar:\")\n",
    "print(\"- Competências técnicas requeridas vs Conhecimentos técnicos\")\n",
    "print(\"- Nível acadêmico (vaga vs candidato)\")\n",
    "print(\"- Nível de idiomas\")\n",
    "print(\"- Principais atividades\")\n",
    "print(\"- Features categóricas originais\")\n",
    "\n",
    "def create_enhanced_features(df):\n",
    "    \"\"\"Cria features aprimoradas usando todas as informações disponíveis\"\"\"\n",
    "    \n",
    "    # Verificar colunas disponíveis\n",
    "    print(f\"Colunas disponíveis no DataFrame: {list(df.columns)}\")\n",
    "    \n",
    "    # 1. Match de competências técnicas\n",
    "    def calculate_tech_match(row):\n",
    "        \"\"\"Calcula match entre competências requeridas e conhecimentos técnicos\"\"\"\n",
    "        competencias_req = str(row.get('competencias_tecnicas_requeridas', '')).lower()\n",
    "        conhecimentos = str(row.get('conhecimentos_tecnicos', '')).lower()\n",
    "        \n",
    "        if not competencias_req or competencias_req == 'nan':\n",
    "            return 0.0\n",
    "        if not conhecimentos or conhecimentos == 'nan':\n",
    "            return 0.0\n",
    "            \n",
    "        # Palavras-chave técnicas\n",
    "        comp_words = set(competencias_req.split())\n",
    "        conhec_words = set(conhecimentos.split())\n",
    "        \n",
    "        if len(comp_words) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Interseção / união\n",
    "        intersection = comp_words.intersection(conhec_words)\n",
    "        match_score = len(intersection) / len(comp_words)\n",
    "        \n",
    "        return round(match_score, 2)\n",
    "    \n",
    "    # 2. Match de nível acadêmico\n",
    "    def calculate_academic_match(row):\n",
    "        \"\"\"Verifica compatibilidade do nível acadêmico\"\"\"\n",
    "        # Usar nomes corretos das colunas após merge\n",
    "        nivel_vaga = str(row.get('nivel_academico', '')).lower()  # Da vaga\n",
    "        nivel_candidato = str(row.get('nivel_academico_candidato', \n",
    "                                   row.get('nivel_academico', ''))).lower()  # Do candidato\n",
    "        \n",
    "        # Hierarquia acadêmica\n",
    "        hierarchy = {\n",
    "            'ensino médio': 1,\n",
    "            'técnico': 2,\n",
    "            'tecnólogo': 3,\n",
    "            'superior': 4,\n",
    "            'pós-graduação': 5,\n",
    "            'mestrado': 6,\n",
    "            'doutorado': 7\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(nivel_vaga, 0)\n",
    "        candidato_level = hierarchy.get(nivel_candidato, 0)\n",
    "        \n",
    "        if vaga_level == 0 or candidato_level == 0:\n",
    "            return 'indefinido'\n",
    "        elif candidato_level >= vaga_level:\n",
    "            return 'compatível'\n",
    "        else:\n",
    "            return 'insuficiente'\n",
    "    \n",
    "    # 3. Match de inglês\n",
    "    def calculate_english_match(row):\n",
    "        \"\"\"Verifica compatibilidade do nível de inglês\"\"\"\n",
    "        # Usar nomes corretos das colunas após merge\n",
    "        nivel_vaga = str(row.get('nivel_ingles', '')).lower()  # Da vaga\n",
    "        nivel_candidato = str(row.get('nivel_ingles_candidato',\n",
    "                                   row.get('nivel_ingles', ''))).lower()  # Do candidato\n",
    "        \n",
    "        # Hierarquia de inglês\n",
    "        hierarchy = {\n",
    "            'nenhum': 0,\n",
    "            'básico': 1,\n",
    "            'intermediário': 2,\n",
    "            'avançado': 3,\n",
    "            'fluente': 4,\n",
    "            'nativo': 5\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(nivel_vaga, -1)\n",
    "        candidato_level = hierarchy.get(nivel_candidato, -1)\n",
    "        \n",
    "        if vaga_level == -1 or candidato_level == -1:\n",
    "            return 'indefinido'\n",
    "        elif candidato_level >= vaga_level:\n",
    "            return 'compatível'\n",
    "        else:\n",
    "            return 'insuficiente'\n",
    "    \n",
    "    # Aplicar as funções\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    print(\"Calculando match de competências técnicas...\")\n",
    "    df_enhanced['tech_match_score'] = df.apply(calculate_tech_match, axis=1)\n",
    "    \n",
    "    print(\"Calculando compatibilidade acadêmica...\")\n",
    "    df_enhanced['academic_match'] = df.apply(calculate_academic_match, axis=1)\n",
    "    \n",
    "    print(\"Calculando compatibilidade de inglês...\")\n",
    "    df_enhanced['english_match'] = df.apply(calculate_english_match, axis=1)\n",
    "    \n",
    "    # 4. Criar texto combinado para análise semântica\n",
    "    def create_combined_text(row):\n",
    "        \"\"\"Combina textos relevantes para análise\"\"\"\n",
    "        parts = [\n",
    "            str(row.get('competencias_tecnicas_requeridas', '')),\n",
    "            str(row.get('conhecimentos_tecnicos', '')),\n",
    "            str(row.get('principais_atividades', '')),\n",
    "            str(row.get('area_de_atuacao', ''))\n",
    "        ]\n",
    "        return ' '.join([p for p in parts if p and p != 'nan']).lower().strip()\n",
    "    \n",
    "    df_enhanced['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Aplicar melhorias no dataset preparado\n",
    "df_enhanced = create_enhanced_features(df_features)\n",
    "\n",
    "print(f\"\\nDataset aprimorado: {df_enhanced.shape}\")\n",
    "print(\"\\nNovas features criadas:\")\n",
    "print(\"- tech_match_score: Score de match técnico (0.0 a 1.0)\")\n",
    "print(\"- academic_match: Compatibilidade acadêmica\")\n",
    "print(\"- english_match: Compatibilidade de inglês\")\n",
    "print(\"- combined_text: Texto combinado para análise semântica\")\n",
    "\n",
    "# Verificar as novas features\n",
    "print(f\"\\n=== ESTATÍSTICAS DAS NOVAS FEATURES ===\")\n",
    "print(f\"Tech match score - Média: {df_enhanced['tech_match_score'].mean():.2f}\")\n",
    "print(f\"Tech match score - Range: {df_enhanced['tech_match_score'].min():.2f} a {df_enhanced['tech_match_score'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nDistribuição Academic Match:\")\n",
    "print(df_enhanced['academic_match'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribuição English Match:\")\n",
    "print(df_enhanced['english_match'].value_counts())\n",
    "\n",
    "print(f\"\\nAmostra dos dados aprimorados:\")\n",
    "sample_cols = ['nome_candidato', 'titulo_vaga', 'tech_match_score', 'academic_match', 'english_match', 'contratado']\n",
    "available_cols = [col for col in sample_cols if col in df_enhanced.columns]\n",
    "if available_cols:\n",
    "    print(df_enhanced[available_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e255d8",
   "metadata": {},
   "source": [
    "## 5. Pipeline de Machine Learning\n",
    "\n",
    "Agora vamos criar um pipeline completo que combina:\n",
    "- **Pré-processamento automático** de features numéricas, categóricas e texto\n",
    "- **Modelo Random Forest** otimizado para o problema de classificação\n",
    "- **Validação cruzada** para avaliar a performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIANDO PIPELINE DE MACHINE LEARNING COMPLETO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CRIANDO PIPELINE COMPLETO ===\")\n",
    "\n",
    "# Selecionar features para o modelo\n",
    "def select_features_for_model(df):\n",
    "    \"\"\"Seleciona features para o modelo\"\"\"\n",
    "    \n",
    "    # Features numéricas\n",
    "    numeric_features = ['tech_match_score']\n",
    "    \n",
    "    # Features categóricas\n",
    "    categorical_features = [\n",
    "        'nivel_profissional',\n",
    "        'areas_atuacao', \n",
    "        'area_de_atuacao',\n",
    "        'academic_match',\n",
    "        'english_match'\n",
    "    ]\n",
    "    \n",
    "    # Feature de texto\n",
    "    text_features = ['combined_text']\n",
    "    \n",
    "    # Verificar quais features existem no dataframe\n",
    "    available_numeric = [f for f in numeric_features if f in df.columns]\n",
    "    available_categorical = [f for f in categorical_features if f in df.columns]\n",
    "    available_text = [f for f in text_features if f in df.columns]\n",
    "    \n",
    "    print(f\"Features numéricas disponíveis: {available_numeric}\")\n",
    "    print(f\"Features categóricas disponíveis: {available_categorical}\")\n",
    "    print(f\"Features de texto disponíveis: {available_text}\")\n",
    "    \n",
    "    return available_numeric, available_categorical, available_text\n",
    "\n",
    "# Preparar dados para treinamento\n",
    "numeric_cols, categorical_cols, text_cols = select_features_for_model(df_enhanced)\n",
    "\n",
    "# Criar features X e target y\n",
    "all_feature_cols = numeric_cols + categorical_cols + text_cols\n",
    "X_enhanced = df_enhanced[all_feature_cols].copy()\n",
    "y_enhanced = df_enhanced['contratado'].copy()\n",
    "\n",
    "print(f\"\\nDataset para treinamento: {X_enhanced.shape}\")\n",
    "print(f\"Distribuição do target: {y_enhanced.value_counts().to_dict()}\")\n",
    "\n",
    "# Criar transformadores\n",
    "print(\"\\nCriando transformadores...\")\n",
    "\n",
    "# Para features numéricas\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Para features categóricas\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# Para features de texto\n",
    "text_transformer = TfidfVectorizer(\n",
    "    max_features=50,  # Limitado para dataset pequeno\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,  # Aceitar termos que aparecem pelo menos 1 vez\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Criar ColumnTransformer\n",
    "transformers = []\n",
    "\n",
    "if numeric_cols:\n",
    "    transformers.append(('num', numeric_transformer, numeric_cols))\n",
    "if categorical_cols:\n",
    "    transformers.append(('cat', categorical_transformer, categorical_cols))\n",
    "if text_cols:\n",
    "    transformers.append(('text', text_transformer, text_cols[0]))  # TfidfVectorizer espera uma string\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError(\"Nenhuma feature válida encontrada!\")\n",
    "\n",
    "preprocessor_enhanced = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Criar pipeline completo\n",
    "pipeline_enhanced = Pipeline([\n",
    "    ('preprocessor', preprocessor_enhanced),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=20,  # Número de árvores\n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Para lidar com desbalanceamento\n",
    "        max_depth=5,  # Profundidade das árvores\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(f\"Pipeline criado com {len(transformers)} tipos de transformadores!\")\n",
    "\n",
    "# Treinar modelo\n",
    "print(\"\\nTreinando modelo...\")\n",
    "try:\n",
    "    pipeline_enhanced.fit(X_enhanced, y_enhanced)\n",
    "    print(\"✅ Modelo treinado com sucesso!\")\n",
    "    \n",
    "    # Fazer predições no conjunto de treino\n",
    "    predictions_enhanced = pipeline_enhanced.predict(X_enhanced)\n",
    "    probabilities_enhanced = pipeline_enhanced.predict_proba(X_enhanced)\n",
    "    \n",
    "    # Avaliar desempenho\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    \n",
    "    accuracy_enhanced = accuracy_score(y_enhanced, predictions_enhanced)\n",
    "    print(f\"\\nAcurácia do modelo: {accuracy_enhanced:.3f}\")\n",
    "    \n",
    "    print(\"\\nRelatório de classificação:\")\n",
    "    print(classification_report(y_enhanced, predictions_enhanced))\n",
    "    \n",
    "    print(\"\\nMatriz de confusão:\")\n",
    "    print(confusion_matrix(y_enhanced, predictions_enhanced))\n",
    "    \n",
    "    # Validação cruzada (se possível)\n",
    "    if len(X_enhanced) >= 3:\n",
    "        try:\n",
    "            cv_scores = cross_val_score(pipeline_enhanced, X_enhanced, y_enhanced, cv=3, scoring='accuracy')\n",
    "            print(f\"\\nValidação cruzada (CV=3): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        except Exception as cv_error:\n",
    "            print(f\"\\nValidação cruzada não possível: {cv_error}\")\n",
    "    \n",
    "    # Mostrar importância das features\n",
    "    print(f\"\\n=== ANÁLISE DAS FEATURES ===\")\n",
    "    try:\n",
    "        feature_importance = pipeline_enhanced.named_steps['classifier'].feature_importances_\n",
    "        print(f\"Número de features após transformação: {len(feature_importance)}\")\n",
    "        \n",
    "        # Features mais importantes\n",
    "        if len(feature_importance) > 0:\n",
    "            top_indices = np.argsort(feature_importance)[-10:][::-1]\n",
    "            print(\"\\nTop 10 features mais importantes:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1:2d}. Feature_{idx:<10} {feature_importance[idx]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Não foi possível calcular importância das features: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro no treinamento do modelo: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c047c8",
   "metadata": {},
   "source": [
    "## 6. Serialização e Teste do Modelo\n",
    "\n",
    "Agora vamos salvar o modelo treinado para uso em produção e testá-lo com exemplos reais para validar seu funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVAR MODELO E CRIAR FUNÇÃO DE TESTE\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"=== SALVANDO MODELO PARA PRODUÇÃO ===\")\n",
    "\n",
    "# Criar diretório se não existir\n",
    "os.makedirs('../app/models', exist_ok=True)\n",
    "\n",
    "# Salvar pipeline treinado\n",
    "model_path = '../app/models/pipeline_aprimorado.joblib'\n",
    "joblib.dump(pipeline_enhanced, model_path)\n",
    "print(f\"✅ Modelo salvo em: {model_path}\")\n",
    "\n",
    "# Salvar metadados do modelo\n",
    "model_metadata = {\n",
    "    'model_version': '2.0_enhanced',\n",
    "    'features': {\n",
    "        'numeric': numeric_cols,\n",
    "        'categorical': categorical_cols,\n",
    "        'text': text_cols\n",
    "    },\n",
    "    'total_features_after_transform': len(pipeline_enhanced.named_steps['classifier'].feature_importances_),\n",
    "    'model_type': 'RandomForestClassifier_Enhanced',\n",
    "    'training_date': str(pd.Timestamp.now()),\n",
    "    'dataset_size': len(X_enhanced),\n",
    "    'accuracy': accuracy_enhanced,\n",
    "    'class_distribution': y_enhanced.value_counts().to_dict(),\n",
    "    'feature_engineering': [\n",
    "        'tech_match_score: Score de compatibilidade técnica (0.0-1.0)',\n",
    "        'academic_match: Compatibilidade acadêmica (compatível/insuficiente/indefinido)',\n",
    "        'english_match: Compatibilidade de inglês (compatível/insuficiente/indefinido)',\n",
    "        'combined_text: Análise semântica de competências e atividades'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_path = '../app/models/model_metadata_enhanced.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Metadados salvos em: {metadata_path}\")\n",
    "\n",
    "# FUNÇÃO DE TESTE DO MODELO\n",
    "def test_model_with_example(vaga_data, candidato_data):\n",
    "    \"\"\"Testa o modelo com dados de entrada\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== TESTE DO MODELO ===\")\n",
    "    print(f\"Vaga: {vaga_data.get('titulo_vaga', 'N/A')}\")\n",
    "    print(f\"Candidato: {candidato_data.get('nome', 'N/A')}\")\n",
    "    \n",
    "    # Simular processamento como seria feito na aplicação\n",
    "    test_row = {**vaga_data, **candidato_data}\n",
    "    \n",
    "    # Aplicar mesma engenharia de features\n",
    "    def calculate_tech_match_test(competencias_req, conhecimentos):\n",
    "        comp_req = str(competencias_req).lower() if competencias_req else ''\n",
    "        conhec = str(conhecimentos).lower() if conhecimentos else ''\n",
    "        \n",
    "        if not comp_req or not conhec:\n",
    "            return 0.0\n",
    "            \n",
    "        comp_words = set(comp_req.split())\n",
    "        conhec_words = set(conhec.split())\n",
    "        \n",
    "        if len(comp_words) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = comp_words.intersection(conhec_words)\n",
    "        return len(intersection) / len(comp_words)\n",
    "    \n",
    "    def calculate_academic_match_test(nivel_vaga, nivel_candidato):\n",
    "        hierarchy = {\n",
    "            'ensino médio': 1, 'técnico': 2, 'tecnólogo': 3,\n",
    "            'superior': 4, 'pós-graduação': 5, 'mestrado': 6, 'doutorado': 7\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(str(nivel_vaga).lower(), 0)\n",
    "        candidato_level = hierarchy.get(str(nivel_candidato).lower(), 0)\n",
    "        \n",
    "        if vaga_level == 0 or candidato_level == 0:\n",
    "            return 'indefinido'\n",
    "        return 'compatível' if candidato_level >= vaga_level else 'insuficiente'\n",
    "    \n",
    "    def calculate_english_match_test(nivel_vaga, nivel_candidato):\n",
    "        hierarchy = {\n",
    "            'nenhum': 0, 'básico': 1, 'intermediário': 2,\n",
    "            'avançado': 3, 'fluente': 4, 'nativo': 5\n",
    "        }\n",
    "        \n",
    "        vaga_level = hierarchy.get(str(nivel_vaga).lower(), -1)\n",
    "        candidato_level = hierarchy.get(str(nivel_candidato).lower(), -1)\n",
    "        \n",
    "        if vaga_level == -1 or candidato_level == -1:\n",
    "            return 'indefinido'\n",
    "        return 'compatível' if candidato_level >= vaga_level else 'insuficiente'\n",
    "    \n",
    "    # Criar features de teste\n",
    "    test_features = {\n",
    "        'tech_match_score': calculate_tech_match_test(\n",
    "            test_row.get('competencias_tecnicas_requeridas'),\n",
    "            test_row.get('conhecimentos_tecnicos')\n",
    "        ),\n",
    "        'nivel_profissional': str(test_row.get('nivel_profissional', '')).lower(),\n",
    "        'areas_atuacao': str(test_row.get('areas_atuacao', '')).lower(),\n",
    "        'area_de_atuacao': str(test_row.get('area_de_atuacao', '')).lower(),\n",
    "        'academic_match': calculate_academic_match_test(\n",
    "            test_row.get('nivel_academico'),\n",
    "            test_row.get('nivel_academico_candidato', test_row.get('nivel_academico'))\n",
    "        ),\n",
    "        'english_match': calculate_english_match_test(\n",
    "            test_row.get('nivel_ingles'),\n",
    "            test_row.get('nivel_ingles_candidato', test_row.get('nivel_ingles'))\n",
    "        ),\n",
    "        'combined_text': ' '.join([\n",
    "            str(test_row.get('competencias_tecnicas_requeridas', '')),\n",
    "            str(test_row.get('conhecimentos_tecnicos', '')),\n",
    "            str(test_row.get('principais_atividades', '')),\n",
    "            str(test_row.get('area_de_atuacao', ''))\n",
    "        ]).lower().strip()\n",
    "    }\n",
    "    \n",
    "    # Criar DataFrame para predição\n",
    "    test_df = pd.DataFrame([test_features])\n",
    "    \n",
    "    # Fazer predição\n",
    "    prediction = pipeline_enhanced.predict(test_df)[0]\n",
    "    probability = pipeline_enhanced.predict_proba(test_df)[0]\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nFeatures calculadas:\")\n",
    "    for key, value in test_features.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n📊 RESULTADO DA PREDIÇÃO:\")\n",
    "    print(f\"   Predição: {'CONTRATADO' if prediction == 1 else 'NÃO CONTRATADO'}\")\n",
    "    print(f\"   Probabilidade de contratação: {probability[1]:.1%}\")\n",
    "    print(f\"   Probabilidade de não contratação: {probability[0]:.1%}\")\n",
    "    print(f\"   Score de compatibilidade: {probability[1]*100:.1f}/100\")\n",
    "    \n",
    "    return {\n",
    "        'prediction': int(prediction),\n",
    "        'probability_hired': float(probability[1]),\n",
    "        'probability_not_hired': float(probability[0]),\n",
    "        'match_score': float(probability[1] * 100),\n",
    "        'features': test_features\n",
    "    }\n",
    "\n",
    "print(\"\\n✅ Modelo salvo e função de teste criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTE PRÁTICO COM EXEMPLO REAL\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTANDO MODELO COM EXEMPLO PRÁTICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dados de teste baseados nos dados reais do dataset\n",
    "vaga_exemplo = {\n",
    "    'titulo_vaga': 'Desenvolvedor Python Sênior',\n",
    "    'competencias_tecnicas_requeridas': 'Python Django Flask PostgreSQL AWS Docker',\n",
    "    'nivel_profissional': 'sênior',\n",
    "    'areas_atuacao': 'TI - Desenvolvimento',\n",
    "    'principais_atividades': 'desenvolvimento backend apis rest microservices',\n",
    "    'nivel_academico': 'superior',\n",
    "    'nivel_ingles': 'intermediário'\n",
    "}\n",
    "\n",
    "candidato_exemplo = {\n",
    "    'nome': 'João Silva',\n",
    "    'conhecimentos_tecnicos': 'Python Django PostgreSQL Docker Kubernetes Git',\n",
    "    'area_de_atuacao': 'Desenvolvimento Web',\n",
    "    'nivel_academico_candidato': 'superior',\n",
    "    'nivel_ingles_candidato': 'avançado'\n",
    "}\n",
    "\n",
    "# Executar teste\n",
    "try:\n",
    "    resultado = test_model_with_example(vaga_exemplo, candidato_exemplo)\n",
    "    \n",
    "    print(f\"\\n🎯 RESUMO DO TESTE:\")\n",
    "    print(f\"   Score Final: {resultado['match_score']:.1f}%\")\n",
    "    print(f\"   Recomendação: {'ALTA compatibilidade' if resultado['match_score'] >= 70 else 'MÉDIA compatibilidade' if resultado['match_score'] >= 50 else 'BAIXA compatibilidade'}\")\n",
    "    \n",
    "    # Teste com candidato menos compatível\n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"TESTE COM CANDIDATO MENOS COMPATÍVEL\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    candidato_menos_compativel = {\n",
    "        'nome': 'Maria Santos',\n",
    "        'conhecimentos_tecnicos': 'Java Spring MySQL',\n",
    "        'area_de_atuacao': 'Desenvolvimento Mobile',\n",
    "        'nivel_academico_candidato': 'técnico',\n",
    "        'nivel_ingles_candidato': 'básico'\n",
    "    }\n",
    "    \n",
    "    resultado2 = test_model_with_example(vaga_exemplo, candidato_menos_compativel)\n",
    "    print(f\"\\n🎯 RESUMO DO TESTE 2:\")\n",
    "    print(f\"   Score Final: {resultado2['match_score']:.1f}%\")\n",
    "    print(f\"   Recomendação: {'ALTA compatibilidade' if resultado2['match_score'] >= 70 else 'MÉDIA compatibilidade' if resultado2['match_score'] >= 50 else 'BAIXA compatibilidade'}\")\n",
    "    \n",
    "    print(f\"\\n✅ TREINAMENTO CONCLUÍDO COM SUCESSO!\")\n",
    "    print(f\"📁 Modelo salvo em: ../app/models/pipeline_aprimorado.joblib\")\n",
    "    print(f\"📋 Metadados salvos em: ../app/models/model_metadata_enhanced.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro no teste: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
